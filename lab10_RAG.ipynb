{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-based Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"A_Review_on_Large_Language_Models_Architectures_Applications_Taxonomies_Open_Issues_and_Challenges.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'A_Review_on_Large_Language_Models_Architectures_Applications_Taxonomies_Open_Issues_and_Challenges.pdf', 'page': 0}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/378289524\\nA Review on Large Language Models: Architectures, Applications,\\nTaxonomies, Open Issues and Challenges\\nArticle\\xa0\\xa0in \\xa0\\xa0IEEE Access · January 2024\\nDOI: 10.1109/ACCESS.2024.3365742\\nCITATIONS\\n154\\nREADS\\n2,578\\n9 authors, including:\\nMohaimenul Azam Khan Raiaan\\nCharles Darwin University\\n11 PUBLICATIONS\\xa0\\xa0\\xa0298 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nSaddam Mukta\\nLappeenranta – Lahti University of Technology LUT\\n90 PUBLICATIONS\\xa0\\xa0\\xa0848 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nKaniz Fatema\\n20 PUBLICATIONS\\xa0\\xa0\\xa0288 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nNur Mohammad Fahad\\nUnited International University\\n13 PUBLICATIONS\\xa0\\xa0\\xa0233 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Mohaimenul Azam Khan Raiaan on 24 February 2024.\\nThe user has requested enhancement of the downloaded file.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Character Text Spitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katar\\AppData\\Local\\Temp\\ipykernel_10768\\1810415143.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "url = os.getenv('QC_URL')\n",
    "api_key = os.getenv('QC_API_KEY')\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=url, \n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECURSIVE_COLLECTION_NAME = \"RecursiveSptitterEmbeddings\"\n",
    "if not qdrant_client.collection_exists(RECURSIVE_COLLECTION_NAME):\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=RECURSIVE_COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=384, distance=models.Distance.COSINE\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, chunk in enumerate(chunks):\n",
    "    text = chunk.page_content\n",
    "    embedding = embeddings_model.embed_query(text)\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=RECURSIVE_COLLECTION_NAME,\n",
    "        points=[{\"id\": index, \"vector\": embedding, \"payload\": {\"text\": text}}]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in documents]\n",
    "semantic_chunker = SemanticChunker(embeddings_model)\n",
    "semantic_chunks = semantic_chunker.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/378289524\\nA Review on Large Language Models: Architectures, Applications,\\nTaxonomies, Open Issues and Challenges\\nArticle\\xa0\\xa0in \\xa0\\xa0IEEE Access · January 2024\\nDOI: 10.1109/ACCESS.2024.3365742\\nCITATIONS\\n154\\nREADS\\n2,578\\n9 authors, including:\\nMohaimenul Azam Khan Raiaan\\nCharles Darwin University\\n11 PUBLICATIONS\\xa0\\xa0\\xa0298 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nSaddam Mukta\\nLappeenranta – Lahti University of Technology LUT\\n90 PUBLICATIONS\\xa0\\xa0\\xa0848 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nKaniz Fatema\\n20 PUBLICATIONS\\xa0\\xa0\\xa0288 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nNur Mohammad Fahad\\nUnited International University\\n13 PUBLICATIONS\\xa0\\xa0\\xa0233 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Mohaimenul Azam Khan Raiaan on 24 February 2024. The user has requested enhancement of the downloaded file.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_COLLECTION_NAME = \"SemanticSptitterEmbeddings\"\n",
    "if not qdrant_client.collection_exists(SEMANTIC_COLLECTION_NAME):\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=SEMANTIC_COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=384, distance=models.Distance.COSINE\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, chunk in enumerate(semantic_chunks):\n",
    "    text = chunk.page_content\n",
    "    embedding = embeddings_model.embed_query(text)\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=SEMANTIC_COLLECTION_NAME,\n",
    "        points=[{\"id\": index, \"vector\": embedding, \"payload\": {\"text\": text}}]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_docs(question, collection_name):\n",
    "    query_embedding = embeddings_model.embed_query(question)\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=5\n",
    "    )\n",
    "    return [hit.payload[\"text\"] for hit in search_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, prompt_template, collection_name):\n",
    "    context_docs = retrieve_relevant_docs(question, collection_name)\n",
    "    context_text = \"\\n\\n\".join(context_docs)\n",
    "    print(f\"\\n{context_text}\\n\")\n",
    "\n",
    "    messages = [\n",
    "        HumanMessage(content=prompt_template.format(context=context_text, question=question))\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What type of data was LLaMA trained on?\",\n",
    "    \"What activation functions are commonly used in LLMs?\",\n",
    "    \"What are the main differences between the BERT model and the GPT model?\",\n",
    "    \"What are the characteristics of the T5 model?\",\n",
    "    \"What are the most significant challenges typical for LLMs?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What type of data was LLaMA trained on?\n",
      "Context:\n",
      "\n",
      "\n",
      "power of 800 GPUs[73]. No information regarding the\n",
      "duration of GPU training is available.\n",
      "MT-NLG: MT-NLG has a huge size of 530 billion\n",
      "parameters. It has been trained on a massive dataset of\n",
      "270 billion tokens, utilizing 4480 80GB A100 GPUs[74].\n",
      "No data regarding the duration of GPU training is available.\n",
      "The model integrates context learning features also.\n",
      "LLaMA: LLaMA is a language model with an enormous\n",
      "capacity with a total of 65 billion parameters. It has\n",
      "undergone pre-training on a large dataset consisting of\n",
      "1.4 trillion tokens. This training process was carried out\n",
      "utilizing 2048 high-performance 80GB A100 GPUs[75]. The\n",
      "training period is explicitly set to 21 days.\n",
      "LLaMA 2: LLaMA 2 is equipped with a total of 70 billion\n",
      "parameters and has performed pre-training on 2 trillion\n",
      "tokens, utilizing 2000 80GB A100 GPUs [76]. The training\n",
      "period is set to 25 days, and the model also contains context-\n",
      "based learning.\n",
      "Falcon: Falcon, equipped with 40 billion parameters,\n",
      "\n",
      "to LLMs training. C4, OpenWebText, and Wikipedia are\n",
      "examples of datasets that belong to the ‘‘Webpages’’ category.\n",
      "At the same time, BookCorpus, Gutenberg, CC-Stories-R,\n",
      "CC-NEWES, and REALNEWS are examples of datasets\n",
      "that belong to the ‘‘Books and News’’ category. These\n",
      "categories reflect the richness and diversity of text data used\n",
      "to train LLMs, including web content, novels, news articles,\n",
      "scientific literature, and codes.\n",
      "From the ✓, we observe that LLaMA has been trained\n",
      "on a wide range of data sources, with significant exposure\n",
      "to webpages (87%), conversation data (5%), books and\n",
      "news (2%), scientific data (3%), and codes (5%). Therefore,\n",
      "LLaMA becomes a versatile model suitable for a wide array\n",
      "of NLP tasks that involve these mentioned data sources.\n",
      "In contrast, GPT-3 and AlphaCode have limited data access\n",
      "of data sources to train their models. GPT-1 and GPT-2\n",
      "focus on webpages (70%) and books & news (30%) data to\n",
      "train the model. GPT-3 is proficient with web pages (84%),\n",
      "\n",
      "review of the recent changes in this area. This article thoroughly overviews LLMs, including their history,\n",
      "architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper\n",
      "begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training\n",
      "phase. Then the paper provides an overview of the existing works, the history of LLMs, their evolution\n",
      "over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different\n",
      "training methods that have been used to train them. The paper also demonstrates the datasets utilized in the\n",
      "studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and\n",
      "healthcare, education, social, business, and agriculture. The study also illustrates how LLMs create an impact\n",
      "on society and shape the future of AI and how they can be used to solve real-world problems. Finally, the\n",
      "\n",
      "as Falcon is a decoder-based model whereas T5 integrated\n",
      "both encoder-decoders. Additionally, Falcon utilizes multi-\n",
      "head query attention to increase the scalability of the model.\n",
      "LLaMA-2 is the updated version of LLaMA. It is an enhanced\n",
      "fine-tuned version that exploits the hardware utilization\n",
      "for efficient training sessions. MT-NLG and PaLM have\n",
      "substantial parameter sizes of 530B and 540B, respectively.\n",
      "Both of them also use the casual decoder technique. However,\n",
      "they have some architectural differences, such as PaLM\n",
      "uses a SwiGLU activation function and adafactor optimizer.\n",
      "Moreover, it uses a higher learning rate and batch size of\n",
      "1× 102 and 1000K. On the contrary, MT-NLG uses a lower\n",
      "learning rate and batch size of 5 × 105 and 64K, respectively.\n",
      "GLM-130B and LaMDA are also effective LLMs, widely\n",
      "used for NLP-based tasks, including question answering, text\n",
      "generation, etc. Both of them use the Gated GLU (GeGLU)\n",
      "activation function, a GLU variant. The following equation is\n",
      "\n",
      "the LLMs domain in several studies [3], [16], [17], [18];\n",
      "but their work still has several limitations. These studies\n",
      "miss many aspects of LLM including high-level architecture\n",
      "and configurations, taxonomies, API and domain-specific\n",
      "applications, and datasets of LLMs. For example, there\n",
      "is a lack of introduction to the core architecture and\n",
      "configurations of the LLMs model, a lack of adequate\n",
      "explanation of the taxonomy of LLMs, differentiation based\n",
      "on ML, domain-specific applications, API applications, and\n",
      "descriptions of LLMs datasets. Furthermore, the vast majority\n",
      "of LLMs review papers are not peer-reviewed works. The\n",
      "absence of these key points in a review indicates that a\n",
      "thorough investigation is missing in the current literature.\n",
      "Due to the significant extent of the constraints, it is possible\n",
      "to mitigate these research gaps by thoroughly analyzing and\n",
      "addressing these missing points. Thus, the motivation of\n",
      "26840 VOLUME 12, 2024\n",
      "\n",
      "Answer:  According to the provided context, LLaMA was pre-trained on a large dataset consisting of 1.4 trillion tokens, which is categorized as \"webpages\" (87%), conversation data (5%), books and news (2%), scientific data (3%), and codes (5%).\n",
      "\n",
      "Question:  What activation functions are commonly used in LLMs?\n",
      "Context:\n",
      "\n",
      "\n",
      "values come from the same encoder-decoder block. Then,\n",
      "Full Attention which is the naive understanding version of\n",
      "self attention, and finally, when the output of encoder block\n",
      "is used as the query of immediate decoder block, is called as\n",
      "cross attention mechanism [9], [50].\n",
      "3) ACTIVATION FUNCTION\n",
      "The activation functions play a vital role in the curve-fitting\n",
      "capacities of LLMs architectures [51]. Several activation\n",
      "functions, such as ReLU, GeLU, and other GLU variations,\n",
      "are explored to determine their performance in current\n",
      "research on LLMs [52], [53].\n",
      "4) NORMALIZATION LAYER\n",
      "Layer normalization is essential for achieving faster conver-\n",
      "gence in LLMs model and emphasizes their effects on stabil-\n",
      "ity during training sessions. It presents different approaches,\n",
      "such as LayerNorm, DeepNorm, and RMSNorm. These\n",
      "layer normalization techniques offer distinct advantages and\n",
      "contribute to the regularization of LLMs applications like\n",
      "GPT-3, BERT, T5, etc., facilitating effective training [54].\n",
      "\n",
      "1) TOKENIZATION\n",
      "The primary emphasis is on tokenization, a crucial prepro-\n",
      "cessing stage of LLMs that involves parsing text into discrete\n",
      "parts referred to as tokens [46]. Characters, subwords,\n",
      "symbols, or words may serve as tokens, contingent upon\n",
      "the language model’s dimensions and nature[47], [48].\n",
      "Various tokenization algorithms are utilized in LLMs, such\n",
      "as WordPiece, UnigramLM, and Byte Pair Encoding (BPE).\n",
      "This algorithm has distinct technique for tokenizing from the\n",
      "input and then, applied for the specific tasks [47], [48], [49].\n",
      "2) ATTENTION MECHANISM\n",
      "The attention mechanisms used in LLMs is a crucial topic\n",
      "hence it contributes in the improvement of the architecture\n",
      "and performance. This mechanism helps to figure out the\n",
      "representation of input sequences by forming links between\n",
      "various tokens. There are several attention mechanism\n",
      "available namely Self-Attention where all the queries and\n",
      "values come from the same encoder-decoder block. Then,\n",
      "\n",
      "are categorized as Causal decoder (CD), Autoregressive\n",
      "(AR), Encoder-decoder (ED), and Prefix decoder (PD) to\n",
      "illustrate architectural diversity. Activation functions vary,\n",
      "influencing the models’ expressive strength from GeLU in\n",
      "GPT-3 to SwiGLU in LLaMA and LLaMA-2. All versions\n",
      "of GPT employ the GeLU as its activation function as it\n",
      "mitigates the vanishing gradient problem and facilitates the\n",
      "generation of smoother gradients throughout the training\n",
      "process. The utilization of SwiGLU as the activation function\n",
      "is observed in models such as PaLM and LLaMA versions\n",
      "1 and 2, as it has gating mechanisms that enhance its ability\n",
      "to capture intricate correlations within the data. Models like\n",
      "BERT, OPT, and T5 use ReLU as the activation function. The\n",
      "Formula of these activation functions are given below[6],\n",
      "[59]:\n",
      "ReLU(x) = max(0, x) = f (x) =\n",
      "{\n",
      "x, if x ≥ 0\n",
      "0, if x < 0 (1)\n",
      "GeLU(x) = 0.5x(tanh[\n",
      "√\n",
      "2/π(x + 0.44715x3)]) (2)\n",
      "SwiGLU(x) = x.Sigmoid(βx).xV (3)\n",
      "\n",
      "the LLMs domain in several studies [3], [16], [17], [18];\n",
      "but their work still has several limitations. These studies\n",
      "miss many aspects of LLM including high-level architecture\n",
      "and configurations, taxonomies, API and domain-specific\n",
      "applications, and datasets of LLMs. For example, there\n",
      "is a lack of introduction to the core architecture and\n",
      "configurations of the LLMs model, a lack of adequate\n",
      "explanation of the taxonomy of LLMs, differentiation based\n",
      "on ML, domain-specific applications, API applications, and\n",
      "descriptions of LLMs datasets. Furthermore, the vast majority\n",
      "of LLMs review papers are not peer-reviewed works. The\n",
      "absence of these key points in a review indicates that a\n",
      "thorough investigation is missing in the current literature.\n",
      "Due to the significant extent of the constraints, it is possible\n",
      "to mitigate these research gaps by thoroughly analyzing and\n",
      "addressing these missing points. Thus, the motivation of\n",
      "26840 VOLUME 12, 2024\n",
      "\n",
      "• Providing a complete overview of LLMs, including their\n",
      "evolution, classification, and transformer architecture.\n",
      "The history of LLMs provides a brief account of the\n",
      "evaluation from its origins (1940) to the present (2023),\n",
      "as well as a taxonomy of LLMs based on pre-trained and\n",
      "API-based models and major LLMs structures.\n",
      "• Describing the comparison of different pre-trained\n",
      "model designs in LLMs, along with their own systems\n",
      "that show how the model architectures are different.\n",
      "• Explaining the influence of ML models on LLMs,\n",
      "demonstrating the significance of ML in various LLMs\n",
      "domains.\n",
      "• Providing a brief overview of the datasets used in the\n",
      "training phase to differentiate between the models in\n",
      "existing works.\n",
      "• Presenting a thorough explanation of the hardware\n",
      "implementation in training and testing models in terms\n",
      "of LLMs.\n",
      "• Defining insights into the potential of LLMs and their\n",
      "impact on society and demonstrating bio-medical appli-\n",
      "\n",
      "Answer:  According to the provided context, several activation functions are explored in current research on Large Language Models (LLMs). Specifically, the following activation functions are mentioned:\n",
      "\n",
      "1. ReLU (Rectified Linear Unit): Used by models like BERT, OPT, and T5.\n",
      "2. GeLU (Gaussian Error Linear Unit): Used by GPT-3 and mitigates the vanishing gradient problem.\n",
      "3. SwiGLU: Used in PaLM and LLaMA versions 1 and 2, with gating mechanisms that enhance its ability to capture intricate correlations within the data.\n",
      "\n",
      "These are the activation functions mentioned in the context as being commonly used in LLMs.\n",
      "\n",
      "Question:  What are the main differences between the BERT model and the GPT model?\n",
      "Context:\n",
      "\n",
      "\n",
      "output depends on both the previous and next tokens, making\n",
      "BERT a bidirectional language model. The smaller variant of\n",
      "BERT consists of 12 encoder blocks with a model dimension\n",
      "of 768 and a parameter count that is approximately equal to\n",
      "that of GPT. In contrast, the larger variant has 24 encoder\n",
      "blocks with a model dimension of 1024 and 336 million\n",
      "parameters [66].\n",
      "In contrast to encoder-only models such as BERT and\n",
      "decoder-only models like GPT-1 and GPT-2, T5 pre-train\n",
      "VOLUME 12, 2024 26851\n",
      "\n",
      "The pre-trained GPT model consists of 12 transformer blocks,\n",
      "each with a d(model) value of 768 and a total of 110 million\n",
      "parameters. GPT-2, the second version of GPT, employs\n",
      "the transformer decoder architecture like GPT-1[66]. GPT-\n",
      "2 employs 50,257 BPE tokens and ensures that the masked\n",
      "multi-head component is preceded by the Layer Norm.\n",
      "In GPT-2, an additional layer norm is included subsequent\n",
      "to the last block. There are four pre-trained GPT-2 models\n",
      "available, each with a unique quantity of decoder blocks.\n",
      "The largest model, which has a d(model) value of 1600 and\n",
      "48 blocks, comprises a total of 1.5 billion model parameters.\n",
      "BERT employs the transformer encoder structure, in contrast\n",
      "to the Transformer decoder structure utilized by GPT-1 and\n",
      "GPT-2[83]. Following the final encoder block is composed of\n",
      "two fully connected output layers separated by a Layer Norm\n",
      "component. The calculation of the likelihood of each token’s\n",
      "output depends on both the previous and next tokens, making\n",
      "\n",
      "model.\n",
      "GPT-3: GPT-3 uses Nvidia A100 GPUs to pre-train on\n",
      "a large 300 billion token set, generating around 175 billion\n",
      "parameters[65]. GPT-3 has context learning features which\n",
      "enables itself to understand the words reasoning, sentence,\n",
      "and language properly.\n",
      "BERT: Trained on an unspecified data scale, the BERT\n",
      "model has a variable number of parameters that depends\n",
      "on batch size and the corresponding model’s hidden layer\n",
      "numbers which is around 340 million. Nvidia A100 and\n",
      "V100 GPUs are used for training, and the length of the\n",
      "training depends on the scale of the model’s parameters [66].\n",
      "Contextual learning is incorporated in the model also.\n",
      "RoBERTa: RoBERTa, an enhanced version of BERT\n",
      "which has a parameter count of 340 million and conducts pre-\n",
      "training on a specific amount of data. The training process\n",
      "completed on 6144 TPU v4 units, running for around a\n",
      "duration of two weeks [67]. The model also contains a context\n",
      "learning feature.\n",
      "T5: T5 uses 1024 TPU v3 units and has a number of\n",
      "\n",
      "Bing, ChatpGPT, and BERT have developed in order to\n",
      "contribute jointly to the industry and academia. Since in\n",
      "the literature, we find a scarcity of adequate data pertaining\n",
      "to LLMs, we present performance outcomes for diverse\n",
      "tasks to publicly accessible LLMs in Table 10. All GPT\n",
      "series, including GPT-1, GPT-2, GPT-3, GPT-3.5, and GPT-\n",
      "4, are evaluated using a variety of metrics, including the\n",
      "Stanford question answering dataset (SQuAD), language\n",
      "model benchmark (LAMBADA), and general language\n",
      "understanding evaluation (GLUE), as shown in Table 10.\n",
      "GPT-1 obtains a score of 68.4 on the GLUE, while GPT-\n",
      "2, GPT-3, GPT-3.5, and GPT-4 attain scores of 84.6, 93.2,\n",
      "93.5, and 94.4, respectively. GLUE results indicate that\n",
      "GPT-4 outperforms prior versions of GPT. The GPT-4, i.e.,\n",
      "in SQuAD and LAMBDA have scores of 93.6 and 82.4,\n",
      "respectively. As shown in the table, GPT-4 outperforms its\n",
      "predecessors in both LAMBDA and SQuAD. As GPT-4\n",
      "outperforms its predecessors in all three benchmark metrics\n",
      "\n",
      "train the model. GPT-3 is proficient with web pages (84%),\n",
      "literature, and news (16%) but requires additional instruction\n",
      "with conversation data, scientific data, and codes. Diverse\n",
      "range of datasets enables the GPT models to generate more\n",
      "contextual information across various domains. Specifically,\n",
      "the Webpages, books, and news datasets help to employ\n",
      "formal and structured language. Besides, GPT models\n",
      "achieve the capability of responding in an informative and\n",
      "accurate way.\n",
      "AlphaCode, as its name suggests, is solely focused on\n",
      "codes (100%) and does not utilize any other data sources.\n",
      "This feature uniquely distinguish AlphaCode from other\n",
      "models and emphasize the significance of this model for\n",
      "code-based tasks. Bard, Bert, and Pangu models exhibit\n",
      "identical traits, with each of them concentrating on the\n",
      "extensive textual data obtained from webpage contents and\n",
      "books for pretraining the models. Bloom and OPT primarily\n",
      "emphasize on evaluating data from books and websites, such\n",
      "\n",
      "Answer:  Based on the provided context, the main differences between the BERT model and the GPT model are:\n",
      "\n",
      "1. **Architecture**: BERT employs the transformer encoder structure, whereas GPT uses the transformer decoder structure.\n",
      "2. **Training data**: The training data for BERT is unspecified, while GPT-1 and GPT-2 were pre-trained on a large corpus of text (GPT-3 was trained on an even larger 300 billion token set).\n",
      "3. **Parameter count**: BERT has around 340 million parameters, whereas the smaller variant of GPT has approximately 110 million parameters.\n",
      "4. **Contextual learning**: Both models incorporate contextual learning features, but their implementation and effectiveness may differ.\n",
      "\n",
      "These differences reflect distinct design choices made by the researchers who developed these models, which have led to different strengths and capabilities in language understanding and generation tasks.\n",
      "\n",
      "Question:  What are the characteristics of the T5 model?\n",
      "Context:\n",
      "\n",
      "\n",
      "learning feature.\n",
      "T5: T5 uses 1024 TPU v3 units and has a number of\n",
      "11 billion parameters. T5 has been pre-trained over a number\n",
      "of tokens of 1 trillion[68]. There is no information available\n",
      "on GPU training time. It also holds the features of contextual\n",
      "learning which provides a satisfactory result.\n",
      "PaLM: PaLM produces a substantial number of parame-\n",
      "ters, around 540 billion, and it manages the pre-training on\n",
      "a large dataset with a tokens of 780 billion. The pre-training\n",
      "process is carried out utilizing by 6144 TPU v4 units[69].\n",
      "The training period extends for 120 days, and the model also\n",
      "incorporates contextual learning.\n",
      "LaMDA: LaMDA uses 1024 TPU v3 units during the\n",
      "training and the model is pre-trained over 768 billion tokens\n",
      "26850 VOLUME 12, 2024\n",
      "\n",
      "M. A. K. Raiaan et al.: Review on Large Language Models\n",
      "TABLE 8. Various LLMs with configuration details and optimization settings (Here, LR= learning rate, CG= Category, AF= the activation function, bs=\n",
      "batch size, NL= the number of layers, NAH= the number of attention heads, SHS= the size of the hidden states, MCLDT= the maximum context length\n",
      "during training, CD= causal decoder, ED= encoder-decoder, PD= prefix decoder, and AR= autoregressive).\n",
      "[]\n",
      "Chinchilla are also LLMs but possess distinct configurations\n",
      "and challenges. Usually, PanGU is highly effective for the\n",
      "Chinese language, whereas Galactica performs well with\n",
      "repeated data. Chinchilla is a scaling strategy constrained by\n",
      "data limitations and creates efficient resource allocation for\n",
      "training and generating output. Falcon and T5 are compact\n",
      "compared to other LLMs, and both are transformer-based\n",
      "models. However, they have some unique differences, such\n",
      "as Falcon is a decoder-based model whereas T5 integrated\n",
      "\n",
      "as Falcon is a decoder-based model whereas T5 integrated\n",
      "both encoder-decoders. Additionally, Falcon utilizes multi-\n",
      "head query attention to increase the scalability of the model.\n",
      "LLaMA-2 is the updated version of LLaMA. It is an enhanced\n",
      "fine-tuned version that exploits the hardware utilization\n",
      "for efficient training sessions. MT-NLG and PaLM have\n",
      "substantial parameter sizes of 530B and 540B, respectively.\n",
      "Both of them also use the casual decoder technique. However,\n",
      "they have some architectural differences, such as PaLM\n",
      "uses a SwiGLU activation function and adafactor optimizer.\n",
      "Moreover, it uses a higher learning rate and batch size of\n",
      "1× 102 and 1000K. On the contrary, MT-NLG uses a lower\n",
      "learning rate and batch size of 5 × 105 and 64K, respectively.\n",
      "GLM-130B and LaMDA are also effective LLMs, widely\n",
      "used for NLP-based tasks, including question answering, text\n",
      "generation, etc. Both of them use the Gated GLU (GeGLU)\n",
      "activation function, a GLU variant. The following equation is\n",
      "\n",
      "M. A. K. Raiaan et al.: Review on Large Language Models\n",
      "with generative span corruption and an encoder-decoder\n",
      "architecture [84]. T5 models have displayed state-of-the-art\n",
      "performance on a wide variety of NLP tasks, like GLUE\n",
      "and SuperGLUE, and are able to expand up to hundreds of\n",
      "billions of parameters. LLaMA normalizes the input for every\n",
      "transformer sub-layer rather than the output [75]. To increase\n",
      "performance, it employs the RMSNorm normalizing function\n",
      "and the SwiGLU activation function rather than the ReLU.\n",
      "Single models are utilized by LaMDA to execute multiple\n",
      "duties. The model architecture is a decoder-only transformer\n",
      "language model. The Transformer is comprised of 64 layers,\n",
      "a d(model) value of 8192, gated-GELU as the activation\n",
      "function, and relative attention the same as T5 LLMs [70].\n",
      "AlphaCode employs an encoder-decoder transformer archi-\n",
      "tecture in which input tokens are passed to the encoder, and\n",
      "one token is extracted from the decoder until an end-of-code\n",
      "\n",
      "M. A. K. Raiaan et al.: Review on Large Language Models\n",
      "T5, and DistilBERT, have been developed to effectively\n",
      "address diverse tasks across multiple domains [37].\n",
      "Following the advent of transformers, subsequent years\n",
      "saw the development of scaling-up LLMs models through the\n",
      "expansion of training data and parameter counts[20]. OpenAI\n",
      "significantly contributed to the development of LLMs in\n",
      "2018. During the same year, GPT, an additional transformer-\n",
      "based architecture, was developed. Multiple iterations of\n",
      "the GPT models, developed by OpenAI, underwent pre-\n",
      "training using extensive datasets comprising excerpts from\n",
      "the Internet, novels, and various other textual sources [38].\n",
      "The first version of the GPT model was referred to as GPT-\n",
      "1 [39]. The introduction of GPT-1 was a notable progression\n",
      "in the field of NLP. GPT-1 effectively produces words that\n",
      "are contextually appropriate, showcasing the transformative\n",
      "capabilities of transformers in significantly advancing natural\n",
      "\n",
      "Answer:  According to the provided text, the characteristics of the T5 model are:\n",
      "\n",
      "* It uses 1024 TPU v3 units for training.\n",
      "* It has a number of parameters equal to 11 billion.\n",
      "* It has been pre-trained over a large dataset with 1 trillion tokens.\n",
      "* It incorporates contextual learning, which provides satisfactory results.\n",
      "* It is a transformer-based model and has both encoder-decoder architecture.\n",
      "\n",
      "Additionally, it's mentioned that T5 models have displayed state-of-the-art performance on various NLP tasks, such as GLUE and SuperGLUE.\n",
      "\n",
      "Question:  What are the most significant challenges typical for LLMs?\n",
      "Context:\n",
      "\n",
      "\n",
      "papers in this field restricts our potential to perform broad\n",
      "comparisons and evaluations. While endeavoring to offer a\n",
      "broad perspective on LLMs concepts, we recognize that this\n",
      "analysis predominantly focuses on the ground-level concepts\n",
      "of LLMs configurations and applications. Limited resources,\n",
      "time, and page constraints affect the extensive exploration\n",
      "of individual LLMs architectures. Although our goal is\n",
      "not to offer the understanding of single LLMs but instead\n",
      "provide the evolution of LLMs and its application around\n",
      "various domains, however, readers looking for detailed\n",
      "analysis of specific architectures and advanced topics are\n",
      "not thoroughly covered. Furthermore, the impact of the\n",
      "LLMs across various domains, including education, health,\n",
      "and economy, is highlighted, but assessing the practical\n",
      "impacts of LLMs in many domains can be complex and\n",
      "subjective, especially when considering their impact on social\n",
      "aspects.\n",
      "XIII. CONCLUSION\n",
      "\n",
      "evaluating LLMs at both the task and societal levels in\n",
      "order to comprehend potential risks. The paper thoroughly\n",
      "analyzes LLMs evaluation methods, focusing on three critical\n",
      "dimensions: what to evaluate, where to evaluate, and how\n",
      "to evaluate. The research also includes tasks such as natural\n",
      "language processing, reasoning, medical applications, ethics,\n",
      "and education. The article examines evaluation methods and\n",
      "benchmarks for assessing LLMs performance, emphasizing\n",
      "successful and unsuccessful cases. The paper also underlines\n",
      "future challenges in LLMs evaluation and emphasizes the\n",
      "importance of evaluating LLMs as a fundamental discipline\n",
      "to support the development of more competent LLMs.\n",
      "Table1 illustrates the comparison between different review\n",
      "papers based on some fundamental properties such as LLMs\n",
      "models, APIs, datasets, domain specific LLMs, ml-based\n",
      "comparison of LLMs, taxonomy, architectures, performance,\n",
      "hardware specifications for testing and training, and config-\n",
      "\n",
      "review of LLMs research from 2017 to 2023, encompass-\n",
      "ing over 5,000 publications. The study aims to provide\n",
      "researchers, practitioners, and policymakers with an overview\n",
      "of the evolving landscape of LLMs research. The study\n",
      "also tracks research trends during the specified time period,\n",
      "including advancements in fundamental algorithms, major\n",
      "NLP tasks, and applications in disciplines such as medicine,\n",
      "engineering, social sciences, and the humanities. In addition\n",
      "to highlighting the dynamic and rapidly changing nature of\n",
      "LLMs research, the study offers insights into their current\n",
      "status, impact, and potential in the context of scientific\n",
      "and technological advancements. Chang et al. [17] focuses\n",
      "on the assessment of LMMs. Their research examines the\n",
      "increasing prevalence of LLMs in academia and industry\n",
      "due to their exceptional performance in various applica-\n",
      "tions. The study highlights the growing significance of\n",
      "evaluating LLMs at both the task and societal levels in\n",
      "\n",
      "the LLMs domain in several studies [3], [16], [17], [18];\n",
      "but their work still has several limitations. These studies\n",
      "miss many aspects of LLM including high-level architecture\n",
      "and configurations, taxonomies, API and domain-specific\n",
      "applications, and datasets of LLMs. For example, there\n",
      "is a lack of introduction to the core architecture and\n",
      "configurations of the LLMs model, a lack of adequate\n",
      "explanation of the taxonomy of LLMs, differentiation based\n",
      "on ML, domain-specific applications, API applications, and\n",
      "descriptions of LLMs datasets. Furthermore, the vast majority\n",
      "of LLMs review papers are not peer-reviewed works. The\n",
      "absence of these key points in a review indicates that a\n",
      "thorough investigation is missing in the current literature.\n",
      "Due to the significant extent of the constraints, it is possible\n",
      "to mitigate these research gaps by thoroughly analyzing and\n",
      "addressing these missing points. Thus, the motivation of\n",
      "26840 VOLUME 12, 2024\n",
      "\n",
      "hardware specifications for testing and training, and config-\n",
      "urations. Huang et al. [18] lack information on LLMs’ API,\n",
      "dataset, domain-specific LLMs, taxonomy, architectures, and\n",
      "LLMs Configurations. In contrast, Zhao et al., [3] has missing\n",
      "aspects on LLMs’ API, domain-specific LLMs, taxonomy,\n",
      "architecture, and configurations. Moreover, Fan et al.[16] and\n",
      "Chang et al., [17] lack information on LLMs’ API, domain-\n",
      "specific LLMs, taxonomy, architecture, and configurations.\n",
      "On the contrary, our paper offers a considerably broader\n",
      "aspects on the LLMs context. In addition to incorporating\n",
      "26842 VOLUME 12, 2024\n",
      "\n",
      "Answer:  According to the provided text, the research highlights several challenges in evaluating and understanding Large Language Models (LLMs). Some of the key challenges mentioned include:\n",
      "\n",
      "1. **Evaluating LLMs at both task and societal levels**: The paper emphasizes the importance of considering both the technical performance of LLMs and their potential impact on society.\n",
      "2. **Complexity of assessing practical impacts**: The text notes that evaluating the practical impacts of LLMs in various domains, such as education, health, and economy, can be complex and subjective, especially when considering social aspects.\n",
      "3. **Limited resources, time, and page constraints**: The authors acknowledge that these limitations restrict their ability to perform broad comparisons and evaluations, and to cover individual LLM architectures in detail.\n",
      "4. **Lack of thorough investigation in current literature**: The paper highlights the absence of key points in many review papers, such as introductions to core architecture and configurations, taxonomies, API applications, domain-specific applications, and dataset descriptions.\n",
      "\n",
      "These challenges suggest that there is a need for more comprehensive and nuanced evaluation methods for LLMs, as well as a deeper understanding of their potential impacts on society.\n"
     ]
    }
   ],
   "source": [
    "rag_template = \"\"\"You are a helpful assistant specialized in answering questions about Large Language Models (LLMs).\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template_rag = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "recursive_splitter_answers = []\n",
    "\n",
    "for question in questions:\n",
    "\n",
    "    print(\"\\nQuestion: \", question)\n",
    "    print(\"Context:\\n\")\n",
    "    answer = generate_response(question, prompt_template_rag, RECURSIVE_COLLECTION_NAME)\n",
    "    recursive_splitter_answers.append(answer)\n",
    "    print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_llm(question, prompt_template):\n",
    "    messages = [\n",
    "        HumanMessage(content=prompt_template.format(question=question))\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What type of data was LLaMA trained on?\n",
      "Answer:  Llama is a large language model developed by Meta, and it was trained on a massive dataset of text from various sources, including but not limited to:\n",
      "\n",
      "* Web pages\n",
      "* Books\n",
      "* Articles\n",
      "* Research papers\n",
      "* User-generated content (e.g., forums, social media)\n",
      "\n",
      "The specific details about the training data are not publicly disclosed by Meta, but it's known that Llama was trained on a large corpus of text in multiple languages.\n",
      "\n",
      "Question:  What activation functions are commonly used in LLMs?\n",
      "Answer:  Commonly used activation functions in Large Language Models (LLMs) include:\n",
      "\n",
      "1. ReLU (Rectified Linear Unit): The most widely used activation function, which sets all negative values to zero.\n",
      "2. GELU (Gaussian Error Linear Unit): A smooth and differentiable variant of ReLU, which is often used in transformer-based models.\n",
      "3. Swish: A self-gated activation function that has been shown to improve performance on certain tasks.\n",
      "\n",
      "These activation functions are typically used in the hidden layers of LLMs, as they help introduce non-linearity into the model's output and facilitate learning complex patterns in data.\n",
      "\n",
      "Question:  What are the main differences between the BERT model and the GPT model?\n",
      "Answer:  BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are two popular Large Language Models with distinct architectures and objectives.\n",
      "\n",
      "**Main differences:**\n",
      "\n",
      "1. **Architecture:** BERT is based on a multi-task learning approach, using a combination of masked language modeling and next sentence prediction tasks to pre-train the model. GPT, on the other hand, uses a single task: predicting the next word in a sequence.\n",
      "2. **Pre-training objective:** BERT's primary goal is to understand the context and relationships between words, while GPT focuses on generating coherent text by predicting the next word in a sequence.\n",
      "3. **Output format:** BERT typically outputs a set of vector representations for each input token, whereas GPT generates a sequence of tokens as output.\n",
      "4. **Training data:** While both models are pre-trained on large datasets (BERT on BookCorpus and Wikipedia, GPT on WebText), their training objectives and architectures lead to different strengths and applications.\n",
      "\n",
      "These differences reflect the distinct design choices made by their creators, which have contributed to their respective successes in various NLP tasks.\n",
      "\n",
      "Question:  What are the characteristics of the T5 model?\n",
      "Answer:  The T5 (Text-to-Text Transfer Transformer) model is a type of Large Language Model developed by Google. Its key characteristics include:\n",
      "\n",
      "1. **Sequence-to-sequence architecture**: T5 uses a transformer-based encoder-decoder structure, similar to other sequence-to-sequence models.\n",
      "2. **Text-to-text format**: Unlike other LLMs that are trained on specific tasks (e.g., question-answering or sentiment analysis), T5 is trained on a general text-to-text task, allowing it to perform various NLP tasks.\n",
      "3. **Pre-training with a single objective**: T5 is pre-trained using a single objective: predicting the next token in a sequence, which enables it to learn general language understanding and generation capabilities.\n",
      "4. **High parameter count**: The original T5 model has around 11 billion parameters, making it one of the largest LLMs at its time of release.\n",
      "\n",
      "These characteristics enable T5 to achieve state-of-the-art results on various NLP benchmarks and tasks, including question-answering, sentiment analysis, and text summarization.\n",
      "\n",
      "Question:  What are the most significant challenges typical for LLMs?\n",
      "Answer:  The most significant challenges typical for Large Language Models (LLMs) include:\n",
      "\n",
      "1. **Lack of Common Sense**: LLMs often struggle with understanding real-world context, nuances, and common sense, leading to absurd or unrealistic responses.\n",
      "2. **Limited Domain Knowledge**: While LLMs can be trained on vast amounts of text data, they may not have in-depth knowledge in specific domains or industries.\n",
      "3. **Adversarial Attacks**: LLMs are vulnerable to adversarial attacks, which can manipulate the model's output by feeding it carefully crafted input.\n",
      "4. **Explainability and Transparency**: It is challenging to understand how LLMs arrive at their conclusions, making it difficult to trust their outputs.\n",
      "5. **Bias and Fairness**: LLMs can perpetuate biases present in the training data, leading to unfair or discriminatory outcomes.\n",
      "6. **Scalability and Computational Resources**: Training and deploying large-scale LLMs requires significant computational resources and energy consumption.\n",
      "\n",
      "These challenges highlight the need for ongoing research and development to improve the robustness, reliability, and fairness of Large Language Models.\n"
     ]
    }
   ],
   "source": [
    "template_llm = \"\"\"You are a helpful assistant specialized in answering questions about Large Language Models (LLMs).\n",
    "Answer the following question to the best of your ability. Be precise and concise\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template_llm = ChatPromptTemplate.from_template(template_llm)\n",
    "\n",
    "pure_llm_answers = []\n",
    "\n",
    "for question in questions:\n",
    "    print(\"\\nQuestion: \", question)\n",
    "    answer = generate_response_llm(question, prompt_template_llm)\n",
    "    pure_llm_answers.append(answer)\n",
    "    print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What type of data was LLaMA trained on?\n",
      "Context:\n",
      "\n",
      "\n",
      "This training process was carried out\n",
      "utilizing 2048 high-performance 80GB A100 GPUs[75]. The\n",
      "training period is explicitly set to 21 days. LLaMA 2: LLaMA 2 is equipped with a total of 70 billion\n",
      "parameters and has performed pre-training on 2 trillion\n",
      "tokens, utilizing 2000 80GB A100 GPUs [76]. The training\n",
      "period is set to 25 days, and the model also contains context-\n",
      "based learning. Falcon: Falcon, equipped with 40 billion parameters,\n",
      "undergoes pre-training on a large dataset of 1.3 trillion\n",
      "tokens [77]. No details regarding the duration of GPU training\n",
      "and it also have the context learning features. Chinchilla: Chinchilla is a language model that has\n",
      "70 billion parameters and has been pre-trained on 1.4 trillion\n",
      "tokens [78]. There is no details regarding the duration of GPU\n",
      "training. OPT: OPT, equipped with 175 billion parameters, con-\n",
      "ducts pre-training on 180 billion tokens utilizing 992 A100\n",
      "GPUs with a capacity of 80GB each[79]. No details\n",
      "regarding the duration of GPU training. Galactica: Galactica possesses 120 billion parameters and\n",
      "has undergone pre-training using 106 billion tokens [80]. Details regarding the duration of GPU training are not given. BLOOM: BLOOM has a remarkable capacity of 176 bil-\n",
      "lion parameters and has undergone pre-training on 366 billion\n",
      "tokens utilizing 384 80GB A100 GPUs[55]. The training\n",
      "period lasts for 105 days, and the model incorporates\n",
      "contextual learning. PanGU-a: PanGU-a is a language model that has been pre-\n",
      "trained on a massive amount of data, specifically 1.1 billion,\n",
      "employing 2048 Ascend 910 processing units [81]. It has\n",
      "an impressive parameter count of 207 billion. No details\n",
      "regarding the duration of GPU training. Our comprehensive description helps to understand the\n",
      "hardware specifications and the computational complexity of\n",
      "each model. The researchers also find an opportunity to know\n",
      "about the implementation details of these models and can\n",
      "improve the performance of their studies.\n",
      "\n",
      "K. Raiaan et al.: Review on Large Language Models\n",
      "TABLE 8. Various LLMs with configuration details and optimization settings (Here, LR= learning rate, CG= Category, AF= the activation function, bs=\n",
      "batch size, NL= the number of layers, NAH= the number of attention heads, SHS= the size of the hidden states, MCLDT= the maximum context length\n",
      "during training, CD= causal decoder, ED= encoder-decoder, PD= prefix decoder, and AR= autoregressive). []\n",
      "Chinchilla are also LLMs but possess distinct configurations\n",
      "and challenges. Usually, PanGU is highly effective for the\n",
      "Chinese language, whereas Galactica performs well with\n",
      "repeated data. Chinchilla is a scaling strategy constrained by\n",
      "data limitations and creates efficient resource allocation for\n",
      "training and generating output. Falcon and T5 are compact\n",
      "compared to other LLMs, and both are transformer-based\n",
      "models. However, they have some unique differences, such\n",
      "as Falcon is a decoder-based model whereas T5 integrated\n",
      "both encoder-decoders. Additionally, Falcon utilizes multi-\n",
      "head query attention to increase the scalability of the model. LLaMA-2 is the updated version of LLaMA. It is an enhanced\n",
      "fine-tuned version that exploits the hardware utilization\n",
      "for efficient training sessions. MT-NLG and PaLM have\n",
      "substantial parameter sizes of 530B and 540B, respectively. Both of them also use the casual decoder technique. However,\n",
      "they have some architectural differences, such as PaLM\n",
      "uses a SwiGLU activation function and adafactor optimizer. Moreover, it uses a higher learning rate and batch size of\n",
      "1× 102 and 1000K. On the contrary, MT-NLG uses a lower\n",
      "learning rate and batch size of 5 × 105 and 64K, respectively. GLM-130B and LaMDA are also effective LLMs, widely\n",
      "used for NLP-based tasks, including question answering, text\n",
      "generation, etc. Both of them use the Gated GLU (GeGLU)\n",
      "activation function, a GLU variant. The following equation is\n",
      "used to express the GeGLU operation [99]. GEGLU(x, W , V , b, c) = GELU(xW + b) ⊗ (xV + c) (4)\n",
      "However, there are noticeable differences between GLM-\n",
      "130B and LaMDA in terms of their decoder mechanisms. GLM-130B employs a prefix decoder, whereas LaMDA\n",
      "adopts a casual decoder technique. In addition, the GLM-\n",
      "130B model employs a larger batch size compared to the\n",
      "LaMDA model. In addition, the presence or absence of\n",
      "biased terms in models, such as Falcon, T5, LLaMA 1,2,\n",
      "and Galactica’s ‘‘No,’’ highlights the complexity of the\n",
      "choices made. From 12 for GPT-1 to 118 for PaLM, the\n",
      "number of layers affects a model’s ability to capture intricate\n",
      "patterns. Optimizers are also diverse, with Adam, AdamW,\n",
      "and AdaFactor playing crucial roles. All GPT variants employ\n",
      "Adam as the optimizer, although models such as Galactica,\n",
      "OPT, and Falcon utilize AdamW as their optimizer. Both\n",
      "T5 and PaLM models utilize the Adafactor optimizer in\n",
      "their respective architectures. These variations highlight the\n",
      "significance of selecting models and configurations that are\n",
      "tailored to particular tasks, with performance, computational\n",
      "resources, and task requirements playing a central role.\n",
      "\n",
      "Our extensive systematic review presents\n",
      "a detailed discussion on LLMs which makes a substantial\n",
      "contribution to the field of LLMs research.\n",
      "\n",
      "this paper is to comprehensively explore the current review\n",
      "papers, identify their limitations, and outline the current\n",
      "state-of-the-art methods to address these vital challenges. Therefore, our primary objective is to explore, comprehend,\n",
      "and evaluate LLMs that encompass domains, evolution,\n",
      "classification, the structure of pre-trained models, resources,\n",
      "and real-time applications. Additionally, our comprehensive\n",
      "review discusses open issues and challenges associated with\n",
      "LLMs, including security, ethical, privacy, economic, and\n",
      "environmental considerations. In addition, we present a set\n",
      "of guidelines to explore future research and development\n",
      "in the effective use of LLMs. We hope that this study will\n",
      "contribute to a better understanding and use of LLMs. The\n",
      "list of contributions to this paper is as follows:\n",
      "• Providing a complete overview of LLMs, including their\n",
      "evolution, classification, and transformer architecture. The history of LLMs provides a brief account of the\n",
      "evaluation from its origins (1940) to the present (2023),\n",
      "as well as a taxonomy of LLMs based on pre-trained and\n",
      "API-based models and major LLMs structures. • Describing the comparison of different pre-trained\n",
      "model designs in LLMs, along with their own systems\n",
      "that show how the model architectures are different. • Explaining the influence of ML models on LLMs,\n",
      "demonstrating the significance of ML in various LLMs\n",
      "domains. • Providing a brief overview of the datasets used in the\n",
      "training phase to differentiate between the models in\n",
      "existing works. • Presenting a thorough explanation of the hardware\n",
      "implementation in training and testing models in terms\n",
      "of LLMs. • Defining insights into the potential of LLMs and their\n",
      "impact on society and demonstrating bio-medical appli-\n",
      "cations in five practical domains, including bio-medical\n",
      "and healthcare, education, social media, business, and\n",
      "agriculture. • Investigating LLMs’s diverse set of open issues, chal-\n",
      "lenges, and future opportunities. This section focuses on\n",
      "identifying key challenges and future opportunities that\n",
      "can aid in advancing knowledge in this area. The remaining sections of the paper are organized as\n",
      "depicted in Figure 2. In Section II, the literature review is dis-\n",
      "cussed. Section III illustrates the history of LLMs; Section IV\n",
      "demonstrates the Methodology; Section V explains the clear\n",
      "concept of large language models; Section VI describes the\n",
      "resources of LLMs; Section VII demonstrates the domain-\n",
      "specific applications of LLMs; and Section VIII explains\n",
      "the societal impact of LLMs, Indusrial significance of\n",
      "LLMs is highlighted in SectionIX, Section X discuss the\n",
      "open issues and challenges regarding LLMs, Section XI\n",
      "discusses about the future research directions of LLMs,\n",
      "SectionXII acknowledges the limitation and Section XIII\n",
      "finally concludes the paper. II. LITERATURE REVIEW\n",
      "The growing number of LLMs is an extraordinary develop-\n",
      "ment in the field of AI. In recent years, numerous studies[3],\n",
      "[16], [17], [18] have been conducted to investigate and\n",
      "evaluate their capabilities. Researchers from various fields\n",
      "have contributed on the rise of LLMs, shedding light on\n",
      "their remarkable advancements, diverse applications, and\n",
      "potential to revolutionize tasks from text generation and com-\n",
      "prehension to demonstrating reasoning skills. Collectively,\n",
      "VOLUME 12, 2024 26841\n",
      "\n",
      "She is actively\n",
      "involved in research activities, especially in health\n",
      "informatics, computer vision, machine learning,\n",
      "deep learning, and artificial intelligence-based\n",
      "systems. She has published several research papers\n",
      "in journals (Scopus) and international conferences. NUR MOHAMMAD FAHADreceived the bach-\n",
      "elor’s degree from the Department of Computer\n",
      "Science and Engineering, United International\n",
      "University (UIU), Bangladesh. During the bach-\n",
      "elor’s study, he has contributed to the academic\n",
      "community as an Undergraduate Teaching Assis-\n",
      "tant with the Department of Computer Science\n",
      "and Engineering, UIU. In addition to his teaching\n",
      "role, he has deeply engaged in cutting-edge\n",
      "research across several domains, including com-\n",
      "puter vision, machine learning, deep learning, health informatics, graph\n",
      "theory, and mental health modeling. SADMAN SAKIBreceived the bachelor’s degree\n",
      "in computer science and engineering from the\n",
      "Department of Computer Science and Engineer-\n",
      "ing, United International University, Bangladesh,\n",
      "in 2023. He was a Teaching Assistant of\n",
      "undergraduate students with the Department of\n",
      "Computer Science and Engineering, United Inter-\n",
      "national University. Besides this, he is actively\n",
      "involved in machine learning, deep learning,\n",
      "artificial intelligence, computer vision, and health\n",
      "informatics research. MOST MARUFATUL JANNAT MIMis currently\n",
      "pursuing the degree with the Computer Science\n",
      "and Engineering Department, United International\n",
      "University (UIU). She is actively involved in\n",
      "research activities related to computer vision,\n",
      "deep learning, graph theory, and human–computer\n",
      "interaction. Her passion lies in pioneering inno-\n",
      "vative research in computer science. Apart from\n",
      "studies, she is involved in co-curricular activities\n",
      "with the UIU APP Forum, where she is also the\n",
      "President and demonstrates strong leadership by organizing various seminars\n",
      "and workshops for computer science students. JUBAER AHMAD received the B.Sc. degree\n",
      "in computer science and engineering from\n",
      "United International University (UIU), Dhaka,\n",
      "Bangladesh, in 2022. He is currently a Research\n",
      "Assistant with the IAR Project, UIU. His research\n",
      "interests include computer vision, NLP, big data,\n",
      "and distributed learning. MOHAMMED EUNUS ALI is a Professor with\n",
      "the Department of CSE, Bangladesh University\n",
      "of Engineering and Technology (BUET), where\n",
      "he is also the Group Leader of the Data Science\n",
      "and Engineering Research Laboratory (DataLab). His research papers have published in top ranking\n",
      "journals and conferences, such as theVLDB\n",
      "Journal, IEEE TRANSACTIONS ON KNOWLEDGE AND\n",
      "DATA ENGINEERING, DMKD, Information Systems\n",
      "Journal, WWWJ, DKE, ICDE, CIKM, EDBT,\n",
      "PVLDB, and UbiComp. His research interests include database systems\n",
      "and information management, including spatial databases, practical machine\n",
      "learning, and social media analytics. He served as a Program Committee\n",
      "Member for many prestigious conferences, including SIGMOD, VLDB,\n",
      "AAAI, and SIGSPATIAL. SAMI AZAM is a leading Researcher and a\n",
      "Professor with the Faculty of Science and Technol-\n",
      "ogy, Charles Darwin University, Australia. He is\n",
      "actively involved in the research fields relating\n",
      "to computer vision, signal processing, artificial\n",
      "intelligence, and biomedical engineering. He has a\n",
      "number of publications in peer-reviewed journals\n",
      "and international conference proceedings. 26874 VOLUME 12, 2024\n",
      "View publication stats\n",
      "\n",
      "Answer:  The text does not explicitly state what type of data LLaMA was trained on. However, it is mentioned that the paper discusses large language models (LLMs) and their applications, but it does not provide specific information about LLaMA's training data.\n",
      "\n",
      "However, based on general knowledge, LLaMA is a large language model developed by Meta AI, and like other large language models, it was likely trained on a massive corpus of text data from various sources, including books, articles, websites, and more.\n",
      "\n",
      "Question:  What activation functions are commonly used in LLMs?\n",
      "Context:\n",
      "\n",
      "\n",
      "Our extensive systematic review presents\n",
      "a detailed discussion on LLMs which makes a substantial\n",
      "contribution to the field of LLMs research.\n",
      "\n",
      "INITIAL SEARCHING\n",
      "The research materials employed in this study have been\n",
      "acquired from recognized scientific journals and conferences\n",
      "from January 2020 to August 2023, conducted through the\n",
      "Google Scholar platform. A comprehensive selection of\n",
      "scholarly research articles has been specified, encompassing\n",
      "various reputable academic sources such as IEEE Xplore,\n",
      "ScienceDirect, ACM Digital Library, Wiley Online Library,\n",
      "Springer Link, MDPI, and patents. Initially, 355 papers were\n",
      "selected based on their relevance to the topic and keyword. Table 2 describes the identification technique of the materials\n",
      "from various electronic sources. B. SEARCHING QUERY AND KEYWORDS\n",
      "Using the combination of the appropriate search queries\n",
      "and keywords enlisted in Table 3helps to perform a proper\n",
      "literature search. To conduct a thorough search of the\n",
      "articles for our LLMs-based review work, we encompass the\n",
      "following terms: ‘‘LLMs AND machine learning OR deep\n",
      "learning OR models,’’ ‘‘LLMs AND machine learning OR\n",
      "deep learning OR API,’’ ‘‘LLMs AND machine learning OR\n",
      "deep learning OR Dataset’’, ‘‘LLMs AND natural language\n",
      "processing OR NLP’’ and ‘‘LLMs AND machine learning\n",
      "OR deep learning OR tools.’’ These specific searching\n",
      "techniques help to extract the eligible and quality research\n",
      "papers. C. INCLUSION AND EXCLUSION CRITERIA SET\n",
      "To acquire the final research papers, PRISMA protocols\n",
      "and principles were adhered to formulate a standard set of\n",
      "TABLE 2. Electronic database search. TABLE 3. Search queries used for the review paper. Inclusion Criteria (IC) and Exclusion Criteria (EC). The\n",
      "inclusion criteria define the standards of the paper that need\n",
      "to be included, while the exclusion criteria eliminate articles\n",
      "that do not meet the inclusion scope. Thus, this manual\n",
      "screening process improves the transparency of selection\n",
      "process. Table 4 presents the inclusion and exclusion criteria\n",
      "set for the proposed study. D. PRISMA DIAGRAM\n",
      "Figure 4 depicts the PRISMA flow diagram utilized in\n",
      "selecting papers for the study. It also provides the numbers\n",
      "of included and excluded papers for better understanding. The diagram begins by identifying articles from electronic\n",
      "databases using keywords, queries, resulting in 355 papers. After applying the screening method to exclude duplicated,\n",
      "low-quality, and irrelevant journal papers, the total number\n",
      "of papers for review is reduced to 294. Following a thorough\n",
      "analysis of the titles and abstracts, a total of 207 papers were\n",
      "selected. The final screening method involves the application\n",
      "of inclusion and exclusion criteria. Following this process,\n",
      "a total of 135 papers were ultimately selected for the final\n",
      "review. The process begins with an extensive collection of\n",
      "papers and reduces to the final selection that meets the pre-\n",
      "defined selection criteria for the systematic review. V. LARGE LANGUAGE MODELS\n",
      "Large language models (LLMs) refer to a specific type of\n",
      "AI algorithm that holds the capability to execute a diverse\n",
      "range of NLP tasks. The most common tasks entail text\n",
      "generation, text analysis, translation, sentiment analysis,\n",
      "26846 VOLUME 12, 2024\n",
      "\n",
      "K. Raiaan et al.: Review on Large Language Models\n",
      "TABLE 5. Hardware specifications for the LLMs model. sub-words, as its output. The dependency between the\n",
      "encoder-decoder in a transformer is significant where\n",
      "the encoder processes the input sequence based on\n",
      "the representation, the decoder provides the desired\n",
      "output sequence. In addition, GPT is a decoder-only\n",
      "transformer [63]. The decoder part of GPT uses a\n",
      "masked self-attention mechanism which can process\n",
      "the input sequence without requiring encoder explicitly. Figure 6F demonstrates the decoder component of a\n",
      "transformer. • Linear Layer and Softmax\n",
      "The linear layer is a fully connected neural network\n",
      "layer that transforms the output embedding into a higher-\n",
      "dimensional space. This step is required to convert\n",
      "the output embedding into the original input space. This transformation enhances the expressiveness of the\n",
      "representation, allowing the model to capture more\n",
      "complex patterns and relationships in the data. Besides,\n",
      "the softmax function generates a probability distribution\n",
      "for each output token in the developed vocabulary,\n",
      "allowing us to generate probabilistic output tokens [64]. Figure 6G shows the process by which the features\n",
      "are propagated through a linear layer, followed by the\n",
      "activation of the accurate output probability using the\n",
      "softmax activation function. B. HARDWARE SPECIFICATIONS FOR LARGE LANGUAGE\n",
      "MODELS\n",
      "Understanding the computing resources and training dura-\n",
      "tions needed for various language models is crucial. This\n",
      "estimation helps us in decision-making when choosing a\n",
      "model for specific tasks.\n",
      "\n",
      "This architecture does\n",
      "not use iteration methods. Instead, it employs a focused (i.e.,\n",
      "attention based) approach to determine global input-output\n",
      "dependencies. The model can take input of varying lengths\n",
      "and can change its focus depending on the length of the\n",
      "sequence. As a result, it has become the go-to architecture\n",
      "in many fields, often replacing sophisticated recurrent or\n",
      "convolutional neural networks with much more efficient\n",
      "structure[59]. In this regard, it is particularly important for\n",
      "LLMs applications. Figure 6 illustrates the architecture of\n",
      "the transformer model. Transformer architecture consists of\n",
      "seven main components. A demonstration of each component\n",
      "is shown below. • Inputs and Input Embeddings\n",
      "The ML models utilize tokens, which are units of\n",
      "text like words or sub words, as the training data. However, these models process numbers. Tokenization\n",
      "begins this translation process by dividing down input\n",
      "text into meaningful components. A unique number\n",
      "identification is assigned to each token, connecting\n",
      "the linguistic information to the numerical vector. This\n",
      "numerical format is known as ‘‘input embeddings.’’\n",
      "These input embeddings are numerical representations\n",
      "of words, which ML models may subsequently process. These embeddings function similarly to a dictionary,\n",
      "assisting the model in understanding the meaning of\n",
      "words by arranging them in a mathematical space where\n",
      "comparable phrases are situated close together. The\n",
      "model is trained to generate these embeddings so that\n",
      "vectors of the same size represent words with similar\n",
      "meanings. Figure6A illustrates the input and input\n",
      "embeddings. • Positional Encoding\n",
      "The sequence of words in a sentence frequently\n",
      "conveys important semantic information. The same\n",
      "set of words in a different order conveys completely\n",
      "different meanings. In this regard, understanding the\n",
      "word order in a sentence is essential in NLP to identify\n",
      "the correct utterance meaning. In general, in terms of\n",
      "neural networks, they do not perceive the order of inputs. 26848 VOLUME 12, 2024\n",
      "\n",
      "K. Raiaan et al.: Review on Large Language Models\n",
      "with generative span corruption and an encoder-decoder\n",
      "architecture [84]. T5 models have displayed state-of-the-art\n",
      "performance on a wide variety of NLP tasks, like GLUE\n",
      "and SuperGLUE, and are able to expand up to hundreds of\n",
      "billions of parameters. LLaMA normalizes the input for every\n",
      "transformer sub-layer rather than the output [75]. To increase\n",
      "performance, it employs the RMSNorm normalizing function\n",
      "and the SwiGLU activation function rather than the ReLU. Single models are utilized by LaMDA to execute multiple\n",
      "duties. The model architecture is a decoder-only transformer\n",
      "language model. The Transformer is comprised of 64 layers,\n",
      "a d(model) value of 8192, gated-GELU as the activation\n",
      "function, and relative attention the same as T5 LLMs [70]. AlphaCode employs an encoder-decoder transformer archi-\n",
      "tecture in which input tokens are passed to the encoder, and\n",
      "one token is extracted from the decoder until an end-of-code\n",
      "token is generated [85]. When contrasting encoder-decoder\n",
      "architectures with decoder-only architectures, the encoder-\n",
      "decoder architecture provides the advantage of enabling\n",
      "bidirectional description representation and provides addi-\n",
      "tional flexibility by separating the encoder structure from\n",
      "the decoder. It employs an asymmetric architecture with\n",
      "1536 encoder tokens but only 768 decoder tokens. It makes\n",
      "use of multi-query attention to lower sampling costs. Cache\n",
      "update costs and memory utilization are greatly reduced when\n",
      "all query heads are used but only shared for key and value\n",
      "heads in each attention block. It employed a SentencePiece\n",
      "tokenizer for tokenization, trained on a combination of\n",
      "CodeContests and GitHub data, with a vocabulary size of\n",
      "8,000 tokens. Through the usage of DNNs, all of these LLMs\n",
      "have demonstrated remarkable performance on various NLP\n",
      "tasks like as language understanding and generation. 2) APPLICATIONS OF LLMS USING VARIOUS DNN MODELS\n",
      "Pre-training Transformer models have led to the proposal\n",
      "of LLMs with impressive capacities in addressing a variety\n",
      "of NLP tasks, including question-answering, document\n",
      "summarization, and language translation[3]. Due to their\n",
      "remarkable abilities in basic tasks of language processing\n",
      "and creation, they have completely transformed the fields\n",
      "of NLP and AI. Various DNN models have been employed\n",
      "in different industries, such as technology, healthcare, and\n",
      "retail to increase performance. DNNs have made substantial\n",
      "progress in improving the capabilities of LLMs[87]. DNN\n",
      "models, such as convolutional neural networks (CNNs),\n",
      "recurrent neural networks (RNNs), generative adversarial\n",
      "networks (GANs), capsule networks (CapsNets), transform-\n",
      "ers, and BERT, have been extensively employed in diverse\n",
      "applications of LLMs [94]. Numerous studies [86], [87],\n",
      "[88], [89], [90], [91], [92], [93] suggest that DNN models\n",
      "are utilized in several types of LLMs-based applications to\n",
      "increase task efficiency. Koizumi et al., [86] introduce an innovative method to\n",
      "address the issue of insufficient training data in audio\n",
      "captioning that utilizes a pre-trained LLMs that uses a\n",
      "decoder for generating captions. The findings of the study\n",
      "demonstrate the effectiveness of the proposed methodology in\n",
      "utilizing LLMs for audio captioning. The performance of this\n",
      "proposed approach outperforms the traditional approaches\n",
      "which are trained from the scratch. In a recent study, Fan et al., [87] discuss the significance\n",
      "of recommender systems in web applications and the\n",
      "shortcomings of current DNN approaches in predicting user\n",
      "preferences. They discuss the capacity of LLMs to tackle the\n",
      "challenges in a recommender systems.\n",
      "\n",
      "Answer:  According to the text, the following activation functions are mentioned as being used in various Large Language Models (LLMs):\n",
      "\n",
      "1. Gated-GELU (used in LaMDA)\n",
      "2. SwiGLU (used in LLaMA)\n",
      "\n",
      "ReLU is also mentioned as a commonly used activation function, but it's not specific to LLMs.\n",
      "\n",
      "Note that the text does not provide an exhaustive list of activation functions used in LLMs, but rather mentions some examples from specific models.\n",
      "\n",
      "Question:  What are the main differences between the BERT model and the GPT model?\n",
      "Context:\n",
      "\n",
      "\n",
      "To choose a model that is appropriate\n",
      "for a given task, a clear understanding of the training times\n",
      "and computational resources is mandatory. Table5 shows\n",
      "the hardware specifications, number of parameters, training\n",
      "duration and other configurations of individual LLMs\n",
      "model. GPT-3: GPT-3 uses Nvidia A100 GPUs to pre-train on\n",
      "a large 300 billion token set, generating around 175 billion\n",
      "parameters[65]. GPT-3 has context learning features which\n",
      "enables itself to understand the words reasoning, sentence,\n",
      "and language properly. BERT: Trained on an unspecified data scale, the BERT\n",
      "model has a variable number of parameters that depends\n",
      "on batch size and the corresponding model’s hidden layer\n",
      "numbers which is around 340 million. Nvidia A100 and\n",
      "V100 GPUs are used for training, and the length of the\n",
      "training depends on the scale of the model’s parameters [66]. Contextual learning is incorporated in the model also. RoBERTa: RoBERTa, an enhanced version of BERT\n",
      "which has a parameter count of 340 million and conducts pre-\n",
      "training on a specific amount of data. The training process\n",
      "completed on 6144 TPU v4 units, running for around a\n",
      "duration of two weeks [67]. The model also contains a context\n",
      "learning feature. T5: T5 uses 1024 TPU v3 units and has a number of\n",
      "11 billion parameters. T5 has been pre-trained over a number\n",
      "of tokens of 1 trillion[68]. There is no information available\n",
      "on GPU training time. It also holds the features of contextual\n",
      "learning which provides a satisfactory result. PaLM: PaLM produces a substantial number of parame-\n",
      "ters, around 540 billion, and it manages the pre-training on\n",
      "a large dataset with a tokens of 780 billion. The pre-training\n",
      "process is carried out utilizing by 6144 TPU v4 units[69]. The training period extends for 120 days, and the model also\n",
      "incorporates contextual learning. LaMDA: LaMDA uses 1024 TPU v3 units during the\n",
      "training and the model is pre-trained over 768 billion tokens\n",
      "26850 VOLUME 12, 2024\n",
      "\n",
      "G. PERFORMANCE ANALYSIS OF LLMS\n",
      "LLMs are models that perform the majority of NLP tasks\n",
      "and numerous models such as GPT-1 through GPT-4,\n",
      "Bing, ChatpGPT, and BERT have developed in order to\n",
      "contribute jointly to the industry and academia. Since in\n",
      "the literature, we find a scarcity of adequate data pertaining\n",
      "to LLMs, we present performance outcomes for diverse\n",
      "tasks to publicly accessible LLMs in Table 10. All GPT\n",
      "series, including GPT-1, GPT-2, GPT-3, GPT-3.5, and GPT-\n",
      "4, are evaluated using a variety of metrics, including the\n",
      "Stanford question answering dataset (SQuAD), language\n",
      "model benchmark (LAMBADA), and general language\n",
      "understanding evaluation (GLUE), as shown in Table 10. GPT-1 obtains a score of 68.4 on the GLUE, while GPT-\n",
      "2, GPT-3, GPT-3.5, and GPT-4 attain scores of 84.6, 93.2,\n",
      "93.5, and 94.4, respectively. GLUE results indicate that\n",
      "GPT-4 outperforms prior versions of GPT. The GPT-4, i.e.,\n",
      "in SQuAD and LAMBDA have scores of 93.6 and 82.4,\n",
      "respectively. As shown in the table, GPT-4 outperforms its\n",
      "predecessors in both LAMBDA and SQuAD. As GPT-4\n",
      "outperforms its predecessors in all three benchmark metrics\n",
      "and exhibits robust performance, it can be concluded that\n",
      "GPT-4 is significantly more effective than its predecessors in\n",
      "tasks involving language understanding and language model-\n",
      "ing. The VietNamese High School Graduation Examination\n",
      "(VNHSGE) English dataset was utilized to analyze various\n",
      "LLMs, including GPT-3.5, BingChat, and BARD. Based\n",
      "on the accuracy presented in Table10, it is evident that\n",
      "BingChat LLM outperforms the other two models, achieving\n",
      "an accuracy of 92.4%. LLMs such as ChatGPT and Bing were\n",
      "evaluated using the average intraclass correlation coefficient\n",
      "(ICC) values. The ICC value for Bing was 0.975, whereas\n",
      "ChatGPT has an ICC value of 0.858. The higher mean ICC\n",
      "value indicates that Bing exhibited robust performance and\n",
      "consistency in major NLP tasks. Table10 depicts that, all\n",
      "of the LLMs mentioned in the table have been analyzed\n",
      "and tested on multiple performance metrics and datasets\n",
      "to validate the robustness and reliability of these language\n",
      "models. VI. RESOURCES OF LARGE LANGUAGE MODELS\n",
      "LLMs have a wide range of potential applications and\n",
      "resources available for their development, deployment, and\n",
      "utilization. Figure7 presents an LLM taxonomy that divided\n",
      "into two main branches: i) pre-trained model-based and ii)\n",
      "API-based. This taxonomy allows us to explore these two\n",
      "distinct aspects of LLMs.\n",
      "\n",
      "K. Raiaan et al.: Review on Large Language Models\n",
      "Hence, these model can lead to a more accurate responses\n",
      "in scientific domains. AlphaCode and GLM-130 are the\n",
      "models of choice for code-related tasks, whereas LLaMA\n",
      "and BERT excel in diverse text data applications. Most of\n",
      "the LLMs such as T5, GPT models, Gopher, GLam, PaLM,\n",
      "and BLOOM frequently utilize websource data which helps\n",
      "them to automate various practical tasks such as content\n",
      "creation, data analysis and virtual chatbot for answering the\n",
      "question. On the contrary, some models such as Falcon and\n",
      "different version of GPT models utilize books and news\n",
      "data facilitates in educational application such as document\n",
      "summarization, and article writings. The models trained on\n",
      "scientific data have several use cases in research domain. In addition, Table 9 provides contextual information of the\n",
      "datasets to maintain the transparency of the comparison\n",
      "among models and provide an effective guide to future model\n",
      "implementation. The ‘‘Size’’ and ‘‘Source’’ columns of the\n",
      "Table listed the additional information. The size of datasets\n",
      "ranges from 5GB (BookCorpus) to a huge 800GB (several\n",
      "datasets), indicating the sheer magnitude of data required\n",
      "to train these LLMs. The source information reveals when\n",
      "and where the data were collected, which is essential for\n",
      "understanding the temporal relevance of the training data and\n",
      "potential biases. Table 9 provides a multitude of information\n",
      "regarding the datasets used to train LLMs and how each\n",
      "model leverages these datasets. This information is invaluable\n",
      "for NLP researchers, developers, and practitioners, as it\n",
      "enables them to make informed decisions about which LLMs\n",
      "to use for specific tasks.\n",
      "\n",
      "A. PRETRAINED MODELS\n",
      "Pretrained language models play a pivotal role in NLP\n",
      "tasks due to their ability to encapsulate broad language\n",
      "understanding and generation skills from diverse text sources. They offer a substantial advantage by minimizing the\n",
      "computational resources and data required for fine-tuning\n",
      "specific tasks. There are some of the most common\n",
      "pre-trained LLMs models, which have been depicted in\n",
      "Table11. 1) GENERATIVE PRETRAINED TRANSFORMER (GPT)\n",
      "GPT [65] is an influential breakthrough in AI, particularly\n",
      "in NLP tasks. Developed by OpenAI, GPT leverages the\n",
      "transformer architecture and extensive pre-training on vast\n",
      "internet text data to achieve a deep understanding of human\n",
      "language. This generative model excels at tasks like text gen-\n",
      "eration, translation, question answering, and more, making it\n",
      "a versatile tool across various NLP domains. GPT’s capacity\n",
      "to capture intricate language patterns and context, coupled\n",
      "with its iterative improvements, has profoundly impacted\n",
      "in academia and industry, revolutionizing the landscape of\n",
      "language understanding and generation. 2) BERT\n",
      "BERT[10], short for ‘‘Bidirectional Encoder Representations\n",
      "from Transformers,’’ is a language model with a distinctive\n",
      "approach. Unlike previous models, BERT is designed to pre-\n",
      "train deep bidirectional representations from unlabeled text\n",
      "by considering both left and right context in all layers. This\n",
      "pre-trained BERT model can be fine-tuned with minimal\n",
      "adjustments to create cutting-edge models for various tasks\n",
      "like question answering and language inference, eliminating\n",
      "the need for extensive task-specific modifications. BERT is\n",
      "both conceptually straightforward and remarkably effective,\n",
      "VOLUME 12, 2024 26857\n",
      "\n",
      "Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized\n",
      "BERT pretraining approach,’’ 2019,arXiv:1907.11692. [68] A.\n",
      "\n",
      "Answer:  According to the provided context, here are the main differences between the BERT model and the GPT model:\n",
      "\n",
      "1. **Training data**: The training data for BERT is unspecified in scale, whereas GPT-3 uses a large 300 billion token set.\n",
      "2. **Pre-training approach**: BERT pre-trains deep bidirectional representations from unlabeled text by considering both left and right context in all layers. In contrast, the pre-training approach of GPT is not explicitly mentioned, but it's described as leveraging the transformer architecture and extensive pre-training on vast internet text data.\n",
      "3. **Fine-tuning**: BERT can be fine-tuned with minimal adjustments to create cutting-edge models for various tasks, whereas the context does not provide information about the fine-tuning process of GPT.\n",
      "\n",
      "It's worth noting that these differences are based on a brief summary and might not capture all the nuances between the two models.\n",
      "\n",
      "Question:  What are the characteristics of the T5 model?\n",
      "Context:\n",
      "\n",
      "\n",
      "K. Raiaan et al.: Review on Large Language Models\n",
      "TABLE 8. Various LLMs with configuration details and optimization settings (Here, LR= learning rate, CG= Category, AF= the activation function, bs=\n",
      "batch size, NL= the number of layers, NAH= the number of attention heads, SHS= the size of the hidden states, MCLDT= the maximum context length\n",
      "during training, CD= causal decoder, ED= encoder-decoder, PD= prefix decoder, and AR= autoregressive). []\n",
      "Chinchilla are also LLMs but possess distinct configurations\n",
      "and challenges. Usually, PanGU is highly effective for the\n",
      "Chinese language, whereas Galactica performs well with\n",
      "repeated data. Chinchilla is a scaling strategy constrained by\n",
      "data limitations and creates efficient resource allocation for\n",
      "training and generating output. Falcon and T5 are compact\n",
      "compared to other LLMs, and both are transformer-based\n",
      "models. However, they have some unique differences, such\n",
      "as Falcon is a decoder-based model whereas T5 integrated\n",
      "both encoder-decoders. Additionally, Falcon utilizes multi-\n",
      "head query attention to increase the scalability of the model. LLaMA-2 is the updated version of LLaMA. It is an enhanced\n",
      "fine-tuned version that exploits the hardware utilization\n",
      "for efficient training sessions. MT-NLG and PaLM have\n",
      "substantial parameter sizes of 530B and 540B, respectively. Both of them also use the casual decoder technique. However,\n",
      "they have some architectural differences, such as PaLM\n",
      "uses a SwiGLU activation function and adafactor optimizer. Moreover, it uses a higher learning rate and batch size of\n",
      "1× 102 and 1000K. On the contrary, MT-NLG uses a lower\n",
      "learning rate and batch size of 5 × 105 and 64K, respectively. GLM-130B and LaMDA are also effective LLMs, widely\n",
      "used for NLP-based tasks, including question answering, text\n",
      "generation, etc. Both of them use the Gated GLU (GeGLU)\n",
      "activation function, a GLU variant. The following equation is\n",
      "used to express the GeGLU operation [99]. GEGLU(x, W , V , b, c) = GELU(xW + b) ⊗ (xV + c) (4)\n",
      "However, there are noticeable differences between GLM-\n",
      "130B and LaMDA in terms of their decoder mechanisms. GLM-130B employs a prefix decoder, whereas LaMDA\n",
      "adopts a casual decoder technique. In addition, the GLM-\n",
      "130B model employs a larger batch size compared to the\n",
      "LaMDA model. In addition, the presence or absence of\n",
      "biased terms in models, such as Falcon, T5, LLaMA 1,2,\n",
      "and Galactica’s ‘‘No,’’ highlights the complexity of the\n",
      "choices made. From 12 for GPT-1 to 118 for PaLM, the\n",
      "number of layers affects a model’s ability to capture intricate\n",
      "patterns. Optimizers are also diverse, with Adam, AdamW,\n",
      "and AdaFactor playing crucial roles. All GPT variants employ\n",
      "Adam as the optimizer, although models such as Galactica,\n",
      "OPT, and Falcon utilize AdamW as their optimizer. Both\n",
      "T5 and PaLM models utilize the Adafactor optimizer in\n",
      "their respective architectures. These variations highlight the\n",
      "significance of selecting models and configurations that are\n",
      "tailored to particular tasks, with performance, computational\n",
      "resources, and task requirements playing a central role.\n",
      "\n",
      "K. Raiaan et al.: Review on Large Language Models\n",
      "with generative span corruption and an encoder-decoder\n",
      "architecture [84]. T5 models have displayed state-of-the-art\n",
      "performance on a wide variety of NLP tasks, like GLUE\n",
      "and SuperGLUE, and are able to expand up to hundreds of\n",
      "billions of parameters. LLaMA normalizes the input for every\n",
      "transformer sub-layer rather than the output [75]. To increase\n",
      "performance, it employs the RMSNorm normalizing function\n",
      "and the SwiGLU activation function rather than the ReLU. Single models are utilized by LaMDA to execute multiple\n",
      "duties. The model architecture is a decoder-only transformer\n",
      "language model. The Transformer is comprised of 64 layers,\n",
      "a d(model) value of 8192, gated-GELU as the activation\n",
      "function, and relative attention the same as T5 LLMs [70]. AlphaCode employs an encoder-decoder transformer archi-\n",
      "tecture in which input tokens are passed to the encoder, and\n",
      "one token is extracted from the decoder until an end-of-code\n",
      "token is generated [85]. When contrasting encoder-decoder\n",
      "architectures with decoder-only architectures, the encoder-\n",
      "decoder architecture provides the advantage of enabling\n",
      "bidirectional description representation and provides addi-\n",
      "tional flexibility by separating the encoder structure from\n",
      "the decoder. It employs an asymmetric architecture with\n",
      "1536 encoder tokens but only 768 decoder tokens. It makes\n",
      "use of multi-query attention to lower sampling costs. Cache\n",
      "update costs and memory utilization are greatly reduced when\n",
      "all query heads are used but only shared for key and value\n",
      "heads in each attention block. It employed a SentencePiece\n",
      "tokenizer for tokenization, trained on a combination of\n",
      "CodeContests and GitHub data, with a vocabulary size of\n",
      "8,000 tokens. Through the usage of DNNs, all of these LLMs\n",
      "have demonstrated remarkable performance on various NLP\n",
      "tasks like as language understanding and generation. 2) APPLICATIONS OF LLMS USING VARIOUS DNN MODELS\n",
      "Pre-training Transformer models have led to the proposal\n",
      "of LLMs with impressive capacities in addressing a variety\n",
      "of NLP tasks, including question-answering, document\n",
      "summarization, and language translation[3]. Due to their\n",
      "remarkable abilities in basic tasks of language processing\n",
      "and creation, they have completely transformed the fields\n",
      "of NLP and AI. Various DNN models have been employed\n",
      "in different industries, such as technology, healthcare, and\n",
      "retail to increase performance. DNNs have made substantial\n",
      "progress in improving the capabilities of LLMs[87]. DNN\n",
      "models, such as convolutional neural networks (CNNs),\n",
      "recurrent neural networks (RNNs), generative adversarial\n",
      "networks (GANs), capsule networks (CapsNets), transform-\n",
      "ers, and BERT, have been extensively employed in diverse\n",
      "applications of LLMs [94]. Numerous studies [86], [87],\n",
      "[88], [89], [90], [91], [92], [93] suggest that DNN models\n",
      "are utilized in several types of LLMs-based applications to\n",
      "increase task efficiency. Koizumi et al., [86] introduce an innovative method to\n",
      "address the issue of insufficient training data in audio\n",
      "captioning that utilizes a pre-trained LLMs that uses a\n",
      "decoder for generating captions. The findings of the study\n",
      "demonstrate the effectiveness of the proposed methodology in\n",
      "utilizing LLMs for audio captioning. The performance of this\n",
      "proposed approach outperforms the traditional approaches\n",
      "which are trained from the scratch. In a recent study, Fan et al., [87] discuss the significance\n",
      "of recommender systems in web applications and the\n",
      "shortcomings of current DNN approaches in predicting user\n",
      "preferences. They discuss the capacity of LLMs to tackle the\n",
      "challenges in a recommender systems.\n",
      "\n",
      "K. Raiaan et al.: Review on Large Language Models\n",
      "Hence, these model can lead to a more accurate responses\n",
      "in scientific domains. AlphaCode and GLM-130 are the\n",
      "models of choice for code-related tasks, whereas LLaMA\n",
      "and BERT excel in diverse text data applications. Most of\n",
      "the LLMs such as T5, GPT models, Gopher, GLam, PaLM,\n",
      "and BLOOM frequently utilize websource data which helps\n",
      "them to automate various practical tasks such as content\n",
      "creation, data analysis and virtual chatbot for answering the\n",
      "question. On the contrary, some models such as Falcon and\n",
      "different version of GPT models utilize books and news\n",
      "data facilitates in educational application such as document\n",
      "summarization, and article writings. The models trained on\n",
      "scientific data have several use cases in research domain. In addition, Table 9 provides contextual information of the\n",
      "datasets to maintain the transparency of the comparison\n",
      "among models and provide an effective guide to future model\n",
      "implementation. The ‘‘Size’’ and ‘‘Source’’ columns of the\n",
      "Table listed the additional information. The size of datasets\n",
      "ranges from 5GB (BookCorpus) to a huge 800GB (several\n",
      "datasets), indicating the sheer magnitude of data required\n",
      "to train these LLMs. The source information reveals when\n",
      "and where the data were collected, which is essential for\n",
      "understanding the temporal relevance of the training data and\n",
      "potential biases. Table 9 provides a multitude of information\n",
      "regarding the datasets used to train LLMs and how each\n",
      "model leverages these datasets. This information is invaluable\n",
      "for NLP researchers, developers, and practitioners, as it\n",
      "enables them to make informed decisions about which LLMs\n",
      "to use for specific tasks.\n",
      "\n",
      "K. Raiaan et al.: Review on Large Language Models\n",
      "which generates a total of 137 billion parameters [70]. It requires a total of of 57.7 days during training. GLM-130B: GLM-130B model possesses a total of\n",
      "130 billion parameters and undergoes pre-training on a huge\n",
      "amount of dataset with 400 billion tokens. The training was\n",
      "conducted utilizing 1024 TPU v4 units and the training\n",
      "session lasts for 60 days [71]. Gopher: Gopher is a language model that has been pre-\n",
      "trained over 300 billion tokens and required 4096 TPU\n",
      "v3 for the experiment. It has a total of 280 billion\n",
      "parameters[72]. The GPU training period is precisely stated\n",
      "as 920 hours. Furthermore, the model integrates context\n",
      "learning to demonstrate an effective outcome. Jurassic-1: Jurassic is a model with an impressive capacity\n",
      "of 178 billion parameters. It has been pre-trained on a massive\n",
      "dataset of 300 billion tokens, utilizing the computational\n",
      "power of 800 GPUs[73]. No information regarding the\n",
      "duration of GPU training is available. MT-NLG: MT-NLG has a huge size of 530 billion\n",
      "parameters. It has been trained on a massive dataset of\n",
      "270 billion tokens, utilizing 4480 80GB A100 GPUs[74]. No data regarding the duration of GPU training is available. The model integrates context learning features also. LLaMA: LLaMA is a language model with an enormous\n",
      "capacity with a total of 65 billion parameters. It has\n",
      "undergone pre-training on a large dataset consisting of\n",
      "1.4 trillion tokens.\n",
      "\n",
      "question answering, and other related functions. GPT-3,\n",
      "GPT-4, PaLM, and LaMDA are extensively used transformer-\n",
      "based LLMs models trained on a large amount of textual\n",
      "data. In terms of architectural properties, these models show\n",
      "variations in size and depth. For example, GPT-3 generates\n",
      "parameters of 175 billion, distributed across 96 levels, while\n",
      "PaLM has an even larger parameter number of 540 billion,\n",
      "organized across 106 layers. All of these models have distinct\n",
      "configurations. The configurations of GPT-3 and PaLM\n",
      "differ in terms of their techniques for generating output. LLMs have evaluated several datasets within Wikipedia, code\n",
      "repositories, books, question sets, and social media data. They\n",
      "have demonstrated their ability to execute diverse activities\n",
      "successfully. Consequently, LLMs have drawn significant\n",
      "attention for their effective contribution in different domains,\n",
      "including education, healthcare, media marketing, and other\n",
      "customer services. A particular LLMs program has superior\n",
      "performance in a specific domain compared to others, such\n",
      "as GPT-3, which has gained recognition for its proficiency in\n",
      "generating text styles, whereas LaMDA demonstrates supe-\n",
      "rior performance in providing accurate responses to factual\n",
      "inquiries. LLMs are an emerging technological innovation\n",
      "that holds the potential to bring about transformative changes\n",
      "across various sectors. A. BACKGROUND OF LARGE LANGUAGE MODELS\n",
      "In this section, we present the essential aspects associated. LLM research requires a comprehensive explanation of the\n",
      "crucial concept. Various vital aspects, such as tokenization,\n",
      "encoding technique, layer normalization, etc., are encom-\n",
      "passed in the following background section. 1) TOKENIZATION\n",
      "The primary emphasis is on tokenization, a crucial prepro-\n",
      "cessing stage of LLMs that involves parsing text into discrete\n",
      "parts referred to as tokens [46]. Characters, subwords,\n",
      "symbols, or words may serve as tokens, contingent upon\n",
      "the language model’s dimensions and nature[47], [48]. Various tokenization algorithms are utilized in LLMs, such\n",
      "as WordPiece, UnigramLM, and Byte Pair Encoding (BPE). This algorithm has distinct technique for tokenizing from the\n",
      "input and then, applied for the specific tasks [47], [48], [49]. 2) ATTENTION MECHANISM\n",
      "The attention mechanisms used in LLMs is a crucial topic\n",
      "hence it contributes in the improvement of the architecture\n",
      "and performance. This mechanism helps to figure out the\n",
      "representation of input sequences by forming links between\n",
      "various tokens. There are several attention mechanism\n",
      "available namely Self-Attention where all the queries and\n",
      "values come from the same encoder-decoder block. Then,\n",
      "Full Attention which is the naive understanding version of\n",
      "self attention, and finally, when the output of encoder block\n",
      "is used as the query of immediate decoder block, is called as\n",
      "cross attention mechanism [9], [50]. 3) ACTIVATION FUNCTION\n",
      "The activation functions play a vital role in the curve-fitting\n",
      "capacities of LLMs architectures [51]. Several activation\n",
      "functions, such as ReLU, GeLU, and other GLU variations,\n",
      "are explored to determine their performance in current\n",
      "research on LLMs [52], [53]. 4) NORMALIZATION LAYER\n",
      "Layer normalization is essential for achieving faster conver-\n",
      "gence in LLMs model and emphasizes their effects on stabil-\n",
      "ity during training sessions. It presents different approaches,\n",
      "such as LayerNorm, DeepNorm, and RMSNorm. These\n",
      "layer normalization techniques offer distinct advantages and\n",
      "contribute to the regularization of LLMs applications like\n",
      "GPT-3, BERT, T5, etc., facilitating effective training [54]. 5) TRAINING METHODS AND FRAMEWORKS\n",
      "LLMs training has different distributed methodologies,\n",
      "including data parallelism, pipeline parallelism, tensor par-\n",
      "allelism, model parallelism, and optimizer parallelism[43],\n",
      "[55]. These techniques contribute to understand the practical\n",
      "and expandable training. Additionally, different libraries and\n",
      "frameworks, including Transformers, DeepSpeed, PyTorch,\n",
      "TensorFlow, MXNet, and MindSpore, are used frequently for\n",
      "their training and further implementation[55]. 6) DATA PREPROCESSING\n",
      "The approaches used to preprocess data focus on the\n",
      "significance of quality filtering, data de-duplication and\n",
      "privacy reduction in preparing training data for LLMs. The filtering technique helps to reduce low quality and\n",
      "relevant data. Besides, it reduces the compute complexity\n",
      "by ignoring the useless pattern of the input. Duplicate\n",
      "samples are removed using de-duplication technique which\n",
      "also avoids the overfitting tendency of the model. Finally,\n",
      "privacy reduction ensures the security and compliance\n",
      "of data and upholds the preservation of the personal\n",
      "data. VOLUME 12, 2024 26847\n",
      "\n",
      "Answer:  The text does not specifically mention the characteristics of the T5 model. However, it is mentioned as one of the transformer-based LLMs models trained on a large amount of textual data, along with GPT-3, PaLM, and LaMDA.\n",
      "\n",
      "However, based on general knowledge about the T5 model, here are some of its characteristics:\n",
      "\n",
      "* The T5 (Text-to-Text Transfer Traning) model is a transformer-based language model developed by Google.\n",
      "* It is designed to perform a wide range of natural language processing tasks, including text classification, question answering, and machine translation.\n",
      "* The T5 model uses a sequence-to-sequence architecture, where the input text is encoded into a sequence of vectors, and then decoded back into text.\n",
      "* It has a large vocabulary size and can handle long sequences of text.\n",
      "* The T5 model has been pre-trained on a large corpus of text data, which enables it to learn general language patterns and relationships.\n",
      "\n",
      "Please note that these characteristics are based on general knowledge about the T5 model and may not be specific to the context in which you asked the question.\n",
      "\n",
      "Question:  What are the most significant challenges typical for LLMs?\n",
      "Context:\n",
      "\n",
      "\n",
      "Our extensive systematic review presents\n",
      "a detailed discussion on LLMs which makes a substantial\n",
      "contribution to the field of LLMs research.\n",
      "\n",
      "Additionally, the paper provides insights\n",
      "into available resources for LLMs development and identifies\n",
      "further research and development areas. A recent study by Fan et al. [16] conducted a bibliometric\n",
      "review of LLMs research from 2017 to 2023, encompass-\n",
      "ing over 5,000 publications. The study aims to provide\n",
      "researchers, practitioners, and policymakers with an overview\n",
      "of the evolving landscape of LLMs research. The study\n",
      "also tracks research trends during the specified time period,\n",
      "including advancements in fundamental algorithms, major\n",
      "NLP tasks, and applications in disciplines such as medicine,\n",
      "engineering, social sciences, and the humanities. In addition\n",
      "to highlighting the dynamic and rapidly changing nature of\n",
      "LLMs research, the study offers insights into their current\n",
      "status, impact, and potential in the context of scientific\n",
      "and technological advancements. Chang et al. [17] focuses\n",
      "on the assessment of LMMs. Their research examines the\n",
      "increasing prevalence of LLMs in academia and industry\n",
      "due to their exceptional performance in various applica-\n",
      "tions. The study highlights the growing significance of\n",
      "evaluating LLMs at both the task and societal levels in\n",
      "order to comprehend potential risks. The paper thoroughly\n",
      "analyzes LLMs evaluation methods, focusing on three critical\n",
      "dimensions: what to evaluate, where to evaluate, and how\n",
      "to evaluate. The research also includes tasks such as natural\n",
      "language processing, reasoning, medical applications, ethics,\n",
      "and education. The article examines evaluation methods and\n",
      "benchmarks for assessing LLMs performance, emphasizing\n",
      "successful and unsuccessful cases. The paper also underlines\n",
      "future challenges in LLMs evaluation and emphasizes the\n",
      "importance of evaluating LLMs as a fundamental discipline\n",
      "to support the development of more competent LLMs. Table1 illustrates the comparison between different review\n",
      "papers based on some fundamental properties such as LLMs\n",
      "models, APIs, datasets, domain specific LLMs, ml-based\n",
      "comparison of LLMs, taxonomy, architectures, performance,\n",
      "hardware specifications for testing and training, and config-\n",
      "urations. Huang et al. [18] lack information on LLMs’ API,\n",
      "dataset, domain-specific LLMs, taxonomy, architectures, and\n",
      "LLMs Configurations. In contrast, Zhao et al., [3] has missing\n",
      "aspects on LLMs’ API, domain-specific LLMs, taxonomy,\n",
      "architecture, and configurations. Moreover, Fan et al.[16] and\n",
      "Chang et al., [17] lack information on LLMs’ API, domain-\n",
      "specific LLMs, taxonomy, architecture, and configurations. On the contrary, our paper offers a considerably broader\n",
      "aspects on the LLMs context. In addition to incorporating\n",
      "26842 VOLUME 12, 2024\n",
      "\n",
      "K. Raiaan et al.: Review on Large Language Models\n",
      "TABLE 10. Accuracy of various LLMs on different datasets. FIGURE 7. Taxonomy of LLMs. achieving state-of-the-art results on different NLP tasks. Notable accomplishments include raising the GLUE score to\n",
      "80.5% (an impressive 7.7% absolute improvement), boosting\n",
      "MultiNLI accuracy to 86.7% (a 4.6% absolute improvement),\n",
      "and significantly improving SQuAD v1.1 question answering\n",
      "Test F1 to 93.2 (a 1.5 point absolute improvement) and\n",
      "SQuAD v2.0 Test F1 to 83.1 (a remarkable 5.1 point absolute\n",
      "improvement). 26858 VOLUME 12, 2024\n",
      "\n",
      "question answering, and other related functions. GPT-3,\n",
      "GPT-4, PaLM, and LaMDA are extensively used transformer-\n",
      "based LLMs models trained on a large amount of textual\n",
      "data. In terms of architectural properties, these models show\n",
      "variations in size and depth. For example, GPT-3 generates\n",
      "parameters of 175 billion, distributed across 96 levels, while\n",
      "PaLM has an even larger parameter number of 540 billion,\n",
      "organized across 106 layers. All of these models have distinct\n",
      "configurations. The configurations of GPT-3 and PaLM\n",
      "differ in terms of their techniques for generating output. LLMs have evaluated several datasets within Wikipedia, code\n",
      "repositories, books, question sets, and social media data. They\n",
      "have demonstrated their ability to execute diverse activities\n",
      "successfully. Consequently, LLMs have drawn significant\n",
      "attention for their effective contribution in different domains,\n",
      "including education, healthcare, media marketing, and other\n",
      "customer services. A particular LLMs program has superior\n",
      "performance in a specific domain compared to others, such\n",
      "as GPT-3, which has gained recognition for its proficiency in\n",
      "generating text styles, whereas LaMDA demonstrates supe-\n",
      "rior performance in providing accurate responses to factual\n",
      "inquiries. LLMs are an emerging technological innovation\n",
      "that holds the potential to bring about transformative changes\n",
      "across various sectors. A. BACKGROUND OF LARGE LANGUAGE MODELS\n",
      "In this section, we present the essential aspects associated. LLM research requires a comprehensive explanation of the\n",
      "crucial concept. Various vital aspects, such as tokenization,\n",
      "encoding technique, layer normalization, etc., are encom-\n",
      "passed in the following background section. 1) TOKENIZATION\n",
      "The primary emphasis is on tokenization, a crucial prepro-\n",
      "cessing stage of LLMs that involves parsing text into discrete\n",
      "parts referred to as tokens [46]. Characters, subwords,\n",
      "symbols, or words may serve as tokens, contingent upon\n",
      "the language model’s dimensions and nature[47], [48]. Various tokenization algorithms are utilized in LLMs, such\n",
      "as WordPiece, UnigramLM, and Byte Pair Encoding (BPE). This algorithm has distinct technique for tokenizing from the\n",
      "input and then, applied for the specific tasks [47], [48], [49]. 2) ATTENTION MECHANISM\n",
      "The attention mechanisms used in LLMs is a crucial topic\n",
      "hence it contributes in the improvement of the architecture\n",
      "and performance. This mechanism helps to figure out the\n",
      "representation of input sequences by forming links between\n",
      "various tokens. There are several attention mechanism\n",
      "available namely Self-Attention where all the queries and\n",
      "values come from the same encoder-decoder block. Then,\n",
      "Full Attention which is the naive understanding version of\n",
      "self attention, and finally, when the output of encoder block\n",
      "is used as the query of immediate decoder block, is called as\n",
      "cross attention mechanism [9], [50]. 3) ACTIVATION FUNCTION\n",
      "The activation functions play a vital role in the curve-fitting\n",
      "capacities of LLMs architectures [51]. Several activation\n",
      "functions, such as ReLU, GeLU, and other GLU variations,\n",
      "are explored to determine their performance in current\n",
      "research on LLMs [52], [53]. 4) NORMALIZATION LAYER\n",
      "Layer normalization is essential for achieving faster conver-\n",
      "gence in LLMs model and emphasizes their effects on stabil-\n",
      "ity during training sessions. It presents different approaches,\n",
      "such as LayerNorm, DeepNorm, and RMSNorm. These\n",
      "layer normalization techniques offer distinct advantages and\n",
      "contribute to the regularization of LLMs applications like\n",
      "GPT-3, BERT, T5, etc., facilitating effective training [54]. 5) TRAINING METHODS AND FRAMEWORKS\n",
      "LLMs training has different distributed methodologies,\n",
      "including data parallelism, pipeline parallelism, tensor par-\n",
      "allelism, model parallelism, and optimizer parallelism[43],\n",
      "[55]. These techniques contribute to understand the practical\n",
      "and expandable training. Additionally, different libraries and\n",
      "frameworks, including Transformers, DeepSpeed, PyTorch,\n",
      "TensorFlow, MXNet, and MindSpore, are used frequently for\n",
      "their training and further implementation[55]. 6) DATA PREPROCESSING\n",
      "The approaches used to preprocess data focus on the\n",
      "significance of quality filtering, data de-duplication and\n",
      "privacy reduction in preparing training data for LLMs. The filtering technique helps to reduce low quality and\n",
      "relevant data. Besides, it reduces the compute complexity\n",
      "by ignoring the useless pattern of the input. Duplicate\n",
      "samples are removed using de-duplication technique which\n",
      "also avoids the overfitting tendency of the model. Finally,\n",
      "privacy reduction ensures the security and compliance\n",
      "of data and upholds the preservation of the personal\n",
      "data. VOLUME 12, 2024 26847\n",
      "\n",
      "One of the significant challenges is a\n",
      "shortage of accessible educational resources for pupils to\n",
      "study outside of school. Although online instructional videos\n",
      "are helping to alleviate the problem, society still hopes that\n",
      "AI will deliver individualized teaching services to satisfy\n",
      "the learning demands of each student and increase teaching\n",
      "efficiency. In the light of above discussion, LLMs have the\n",
      "potential to revolutionize many facets of learning, teaching,\n",
      "and educational research in the education sector [140]. The GPT model aids the students in converting the math\n",
      "word problems into representative equations [143]. Kasenci\n",
      "et al., [19] highlighted substantial impact of LLMs in\n",
      "education by facilitating personalized learning, automating\n",
      "grading process, and accessibility of educational resources. Hadi et al.,[137] presents a thorough analysis of LLMs, cov-\n",
      "ering their historical development, wide-ranging applications\n",
      "in domains such as medicine, engineering, education, and\n",
      "their potential impact on the trajectory of AI. Lo et al.,\n",
      "[138] and Dwivedi et.\n",
      "\n",
      "Answer:  I don't see any mention of specific \"most significant challenges\" in the provided text. However, I can summarize some potential challenges mentioned in the text:\n",
      "\n",
      "* Shortage of accessible educational resources for pupils to study outside of school\n",
      "* Overfitting tendency of models due to duplicate samples\n",
      "* Compute complexity and data quality issues\n",
      "* Privacy reduction and security concerns\n",
      "\n",
      "Please note that these are not necessarily the \"most significant\" challenges, but rather some potential ones mentioned in the text.\n"
     ]
    }
   ],
   "source": [
    "semantic_splitter_answers = []\n",
    "for question in questions:\n",
    "\n",
    "    print(\"\\nQuestion: \", question)\n",
    "    print(\"Context:\\n\")\n",
    "    answer = generate_response(question, prompt_template_rag, SEMANTIC_COLLECTION_NAME)\n",
    "    semantic_splitter_answers.append(answer)\n",
    "    print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What type of data was LLaMA trained on?\n",
      "\n",
      "Pure LLM response:\n",
      "Llama is a large language model developed by Meta, and it was trained on a massive dataset of text from various sources, including but not limited to:\n",
      "\n",
      "* Web pages\n",
      "* Books\n",
      "* Articles\n",
      "* Research papers\n",
      "* User-generated content (e.g., forums, social media)\n",
      "\n",
      "The specific details about the training data are not publicly disclosed by Meta, but it's known that Llama was trained on a large corpus of text in multiple languages.\n",
      "\n",
      "RAG with recursive splitter:\n",
      "According to the provided context, LLaMA was pre-trained on a large dataset consisting of 1.4 trillion tokens, which is categorized as \"webpages\" (87%), conversation data (5%), books and news (2%), scientific data (3%), and codes (5%).\n",
      "\n",
      "RAG with semantic splitter:\n",
      "The text does not explicitly state what type of data LLaMA was trained on. However, it is mentioned that the paper discusses large language models (LLMs) and their applications, but it does not provide specific information about LLaMA's training data.\n",
      "\n",
      "However, based on general knowledge, LLaMA is a large language model developed by Meta AI, and like other large language models, it was likely trained on a massive corpus of text data from various sources, including books, articles, websites, and more.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "What activation functions are commonly used in LLMs?\n",
      "\n",
      "Pure LLM response:\n",
      "Commonly used activation functions in Large Language Models (LLMs) include:\n",
      "\n",
      "1. ReLU (Rectified Linear Unit): The most widely used activation function, which sets all negative values to zero.\n",
      "2. GELU (Gaussian Error Linear Unit): A smooth and differentiable variant of ReLU, which is often used in transformer-based models.\n",
      "3. Swish: A self-gated activation function that has been shown to improve performance on certain tasks.\n",
      "\n",
      "These activation functions are typically used in the hidden layers of LLMs, as they help introduce non-linearity into the model's output and facilitate learning complex patterns in data.\n",
      "\n",
      "RAG with recursive splitter:\n",
      "According to the provided context, several activation functions are explored in current research on Large Language Models (LLMs). Specifically, the following activation functions are mentioned:\n",
      "\n",
      "1. ReLU (Rectified Linear Unit): Used by models like BERT, OPT, and T5.\n",
      "2. GeLU (Gaussian Error Linear Unit): Used by GPT-3 and mitigates the vanishing gradient problem.\n",
      "3. SwiGLU: Used in PaLM and LLaMA versions 1 and 2, with gating mechanisms that enhance its ability to capture intricate correlations within the data.\n",
      "\n",
      "These are the activation functions mentioned in the context as being commonly used in LLMs.\n",
      "\n",
      "RAG with semantic splitter:\n",
      "According to the text, the following activation functions are mentioned as being used in various Large Language Models (LLMs):\n",
      "\n",
      "1. Gated-GELU (used in LaMDA)\n",
      "2. SwiGLU (used in LLaMA)\n",
      "\n",
      "ReLU is also mentioned as a commonly used activation function, but it's not specific to LLMs.\n",
      "\n",
      "Note that the text does not provide an exhaustive list of activation functions used in LLMs, but rather mentions some examples from specific models.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "What are the main differences between the BERT model and the GPT model?\n",
      "\n",
      "Pure LLM response:\n",
      "BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are two popular Large Language Models with distinct architectures and objectives.\n",
      "\n",
      "**Main differences:**\n",
      "\n",
      "1. **Architecture:** BERT is based on a multi-task learning approach, using a combination of masked language modeling and next sentence prediction tasks to pre-train the model. GPT, on the other hand, uses a single task: predicting the next word in a sequence.\n",
      "2. **Pre-training objective:** BERT's primary goal is to understand the context and relationships between words, while GPT focuses on generating coherent text by predicting the next word in a sequence.\n",
      "3. **Output format:** BERT typically outputs a set of vector representations for each input token, whereas GPT generates a sequence of tokens as output.\n",
      "4. **Training data:** While both models are pre-trained on large datasets (BERT on BookCorpus and Wikipedia, GPT on WebText), their training objectives and architectures lead to different strengths and applications.\n",
      "\n",
      "These differences reflect the distinct design choices made by their creators, which have contributed to their respective successes in various NLP tasks.\n",
      "\n",
      "RAG with recursive splitter:\n",
      "Based on the provided context, the main differences between the BERT model and the GPT model are:\n",
      "\n",
      "1. **Architecture**: BERT employs the transformer encoder structure, whereas GPT uses the transformer decoder structure.\n",
      "2. **Training data**: The training data for BERT is unspecified, while GPT-1 and GPT-2 were pre-trained on a large corpus of text (GPT-3 was trained on an even larger 300 billion token set).\n",
      "3. **Parameter count**: BERT has around 340 million parameters, whereas the smaller variant of GPT has approximately 110 million parameters.\n",
      "4. **Contextual learning**: Both models incorporate contextual learning features, but their implementation and effectiveness may differ.\n",
      "\n",
      "These differences reflect distinct design choices made by the researchers who developed these models, which have led to different strengths and capabilities in language understanding and generation tasks.\n",
      "\n",
      "RAG with semantic splitter:\n",
      "According to the provided context, here are the main differences between the BERT model and the GPT model:\n",
      "\n",
      "1. **Training data**: The training data for BERT is unspecified in scale, whereas GPT-3 uses a large 300 billion token set.\n",
      "2. **Pre-training approach**: BERT pre-trains deep bidirectional representations from unlabeled text by considering both left and right context in all layers. In contrast, the pre-training approach of GPT is not explicitly mentioned, but it's described as leveraging the transformer architecture and extensive pre-training on vast internet text data.\n",
      "3. **Fine-tuning**: BERT can be fine-tuned with minimal adjustments to create cutting-edge models for various tasks, whereas the context does not provide information about the fine-tuning process of GPT.\n",
      "\n",
      "It's worth noting that these differences are based on a brief summary and might not capture all the nuances between the two models.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "What are the characteristics of the T5 model?\n",
      "\n",
      "Pure LLM response:\n",
      "The T5 (Text-to-Text Transfer Transformer) model is a type of Large Language Model developed by Google. Its key characteristics include:\n",
      "\n",
      "1. **Sequence-to-sequence architecture**: T5 uses a transformer-based encoder-decoder structure, similar to other sequence-to-sequence models.\n",
      "2. **Text-to-text format**: Unlike other LLMs that are trained on specific tasks (e.g., question-answering or sentiment analysis), T5 is trained on a general text-to-text task, allowing it to perform various NLP tasks.\n",
      "3. **Pre-training with a single objective**: T5 is pre-trained using a single objective: predicting the next token in a sequence, which enables it to learn general language understanding and generation capabilities.\n",
      "4. **High parameter count**: The original T5 model has around 11 billion parameters, making it one of the largest LLMs at its time of release.\n",
      "\n",
      "These characteristics enable T5 to achieve state-of-the-art results on various NLP benchmarks and tasks, including question-answering, sentiment analysis, and text summarization.\n",
      "\n",
      "RAG with recursive splitter:\n",
      "According to the provided text, the characteristics of the T5 model are:\n",
      "\n",
      "* It uses 1024 TPU v3 units for training.\n",
      "* It has a number of parameters equal to 11 billion.\n",
      "* It has been pre-trained over a large dataset with 1 trillion tokens.\n",
      "* It incorporates contextual learning, which provides satisfactory results.\n",
      "* It is a transformer-based model and has both encoder-decoder architecture.\n",
      "\n",
      "Additionally, it's mentioned that T5 models have displayed state-of-the-art performance on various NLP tasks, such as GLUE and SuperGLUE.\n",
      "\n",
      "RAG with semantic splitter:\n",
      "The text does not specifically mention the characteristics of the T5 model. However, it is mentioned as one of the transformer-based LLMs models trained on a large amount of textual data, along with GPT-3, PaLM, and LaMDA.\n",
      "\n",
      "However, based on general knowledge about the T5 model, here are some of its characteristics:\n",
      "\n",
      "* The T5 (Text-to-Text Transfer Traning) model is a transformer-based language model developed by Google.\n",
      "* It is designed to perform a wide range of natural language processing tasks, including text classification, question answering, and machine translation.\n",
      "* The T5 model uses a sequence-to-sequence architecture, where the input text is encoded into a sequence of vectors, and then decoded back into text.\n",
      "* It has a large vocabulary size and can handle long sequences of text.\n",
      "* The T5 model has been pre-trained on a large corpus of text data, which enables it to learn general language patterns and relationships.\n",
      "\n",
      "Please note that these characteristics are based on general knowledge about the T5 model and may not be specific to the context in which you asked the question.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "What are the most significant challenges typical for LLMs?\n",
      "\n",
      "Pure LLM response:\n",
      "The most significant challenges typical for Large Language Models (LLMs) include:\n",
      "\n",
      "1. **Lack of Common Sense**: LLMs often struggle with understanding real-world context, nuances, and common sense, leading to absurd or unrealistic responses.\n",
      "2. **Limited Domain Knowledge**: While LLMs can be trained on vast amounts of text data, they may not have in-depth knowledge in specific domains or industries.\n",
      "3. **Adversarial Attacks**: LLMs are vulnerable to adversarial attacks, which can manipulate the model's output by feeding it carefully crafted input.\n",
      "4. **Explainability and Transparency**: It is challenging to understand how LLMs arrive at their conclusions, making it difficult to trust their outputs.\n",
      "5. **Bias and Fairness**: LLMs can perpetuate biases present in the training data, leading to unfair or discriminatory outcomes.\n",
      "6. **Scalability and Computational Resources**: Training and deploying large-scale LLMs requires significant computational resources and energy consumption.\n",
      "\n",
      "These challenges highlight the need for ongoing research and development to improve the robustness, reliability, and fairness of Large Language Models.\n",
      "\n",
      "RAG with recursive splitter:\n",
      "According to the provided text, the research highlights several challenges in evaluating and understanding Large Language Models (LLMs). Some of the key challenges mentioned include:\n",
      "\n",
      "1. **Evaluating LLMs at both task and societal levels**: The paper emphasizes the importance of considering both the technical performance of LLMs and their potential impact on society.\n",
      "2. **Complexity of assessing practical impacts**: The text notes that evaluating the practical impacts of LLMs in various domains, such as education, health, and economy, can be complex and subjective, especially when considering social aspects.\n",
      "3. **Limited resources, time, and page constraints**: The authors acknowledge that these limitations restrict their ability to perform broad comparisons and evaluations, and to cover individual LLM architectures in detail.\n",
      "4. **Lack of thorough investigation in current literature**: The paper highlights the absence of key points in many review papers, such as introductions to core architecture and configurations, taxonomies, API applications, domain-specific applications, and dataset descriptions.\n",
      "\n",
      "These challenges suggest that there is a need for more comprehensive and nuanced evaluation methods for LLMs, as well as a deeper understanding of their potential impacts on society.\n",
      "\n",
      "RAG with semantic splitter:\n",
      "I don't see any mention of specific \"most significant challenges\" in the provided text. However, I can summarize some potential challenges mentioned in the text:\n",
      "\n",
      "* Shortage of accessible educational resources for pupils to study outside of school\n",
      "* Overfitting tendency of models due to duplicate samples\n",
      "* Compute complexity and data quality issues\n",
      "* Privacy reduction and security concerns\n",
      "\n",
      "Please note that these are not necessarily the \"most significant\" challenges, but rather some potential ones mentioned in the text.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(questions)):\n",
    "    print(f\"Question:\\n{questions[i]}\")\n",
    "    print(f\"\\nPure LLM response:\\n{pure_llm_answers[i]}\")\n",
    "    print(f\"\\nRAG with recursive splitter:\\n{recursive_splitter_answers[i]}\")\n",
    "    print(f\"\\nRAG with semantic splitter:\\n{semantic_splitter_answers[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przeprowadzonych eksperymentach odpowiedzi tradycyjnego modelu LLM okazały się najbardziej kompleksowe, uwzględniające najszerszy zakres informacji. Odpowiedzi z wykorzystaniem RAGa ograniczyły się do informacji zawartych w dokumencie. Wnika to z faktu, że informacje, których wymagały postawione pytania mieściły się w zakresie wiedzy modelu językowego. Natomiast przewaga systemów opartch o RAG najbardziej uwidacznia się w przypadku koniecznośći uzyskania odpowiedzi na bardziej specyficzne pytania, które wykraczją poza zakres danych treningowych modelu. Wówczas dzięki zastosowaniu RAGa możliwe jest dostosowanie modelu do specyficznych zadń, poprzez dostarczenie odpowiedniego kontekstu w postaci dokumentów. \n",
    "\n",
    "W przeprowadzonych eksperymentach RAG został przetestowany z dwoma różnymi sposobami chunkowania – Recursive Character Splitter i Semantic Chunker. Druga metoda, bardziej eksperymentalna, wykorzystuje embeddingi do wyodrębniania fragmentów powiązanych semantycznie. W przypadku tej metody kontekst był zazwyczaj dłuższy, jednak nie zawsze uwzględniał najistotniejsze informacje z punktu widzenia odpowiedzi na pytania, co przełożyło się na gorsze rezultaty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "1. How does RAG improve the quality and reliability of LLM responses compared to pure LLM generation?\n",
    "   \n",
    "   Dzięki zastosowaniu RAGa, LLM może korzystać z dodatkowego kontekstu dostarczanego przez zewnętrzne źródła danych, co jest szczególnie przydatne, gdy model potrzebuje dostępu do informacji, które nie były dostępne podczas jego trenowania. RAG umożliwia uzyskanie specyficznych informacji na podstawie wybranych dokumentów, co prowadzi do bardziej precyzyjnych odpowiedzi. Odpowiedzi te nie są ograniczone do wiedzy zakodowanej w modelu podczas trenowania, ale mogą być dynamicznie rozszerzane o aktualne i specjalistyczne dane. Dzięki zastosowaniu RAGa można szybko dostosować LLM do specyficznych zadań bez konieczności czasochłonnego fine-tuningu. \n",
    "2. What are the key factors affecting RAG performance (chunk size, embedding quality, prompt design)?\n",
    "   \n",
    "   Wszystkie wymienione elementy (rozmiar chunków, jakość embeddingów i projektowanie promptów) mają istotne znaczeine dla jakości odpowiedzi generowanych przez model, jednak wydaje się, że w przypadku RAGa proces chunkowania ma szczególne znaczenie. Zarówno rozmiar chunków jak i technika podziału mają duży wpływ na uzyskiwane rezultaty. Zbyt mały rozmiar chunków może sprawić, że kontekst stanie się zbyt krótki, co prowadzi do pominięcia istotnych informacji. Z kolei zbyt duże fragmenty mogą powodować, że model nie uchwyci kluczowych informacji, które są istotne dla generowania odpowiedzi. Ważne jest, aby zastosować odpowiedni splitter, który zapewni, że fragmenty tworzące spójną całość nie zostaną rozdzielone. Należy również pamiętać, że prosty podział dokumentu na fragmenty może nie być wystarczający. Dokumenty często zawierają różnorodne dane, takie jak tabele, wykresy czy obrazy, które są istotne dla treści, ale muszą być najpierw odpowiednio sparsowane, aby mogły być przetworzone przez model. Odpowiednie przygotowanie tych elementów jest kluczowe dla uzyskania wysokiej jakości odpowiedzi w systemie RAG.\n",
    "3. How does the choice of vector database and embedding model impact system performance?\n",
    "   \n",
    "   Jakości modelu embeddingowego wpływa na dokładność odwzorowywania semantycznego znaczenia tekstu. Lepsze embeddingi prowadzą do bardziej trafnych wyników wyszukiwania, co bezpośrednio wpływa na jakość odpowiedzi.\n",
    "\n",
    "   Efektywność wektorowej bazy danych w wyszukiwaniu najbliższych sąsiadów wpływa na czas odpowiedzi systemu. Bazy zoptymalizowane pod kątem szybkiego indeksowania i wyszukiwania zapewniają szybszy dostęp do odpowiednich dokumentów. Bazy danych, które umożliwiają filtrowanie wyników na podstawie metadanych, np. typu dokumentu, zwiększają trafność wyszukiwań. \n",
    "4. What are the main challenges in implementing a production-ready RAG system?\n",
    "\n",
    "   Wśród wyzwań związanych z implementacją systemów RAG w środowisku produkcyjnym można wyróznić długi czas przetwarzania. Zapytania wykonywane z wykorzystaniem RAGa miały kilkukrotnie dłuższy czas przetwrzania w porównianiu z tradycyjnym modelem LLM. Kolejnym istotnym wyzwaniem jest zależność jakości otrzymywanych wyników od chunkowania. Wiedza modelu w przypadku wykorzystania RAGa ogranicza się do informacji zawartych w chunkach, przez co model ma trudności w odpwiadaniu na pytania dotyczące szerszego kontekstu, np. wniosków płynących z całego dokumentu. Istotnym wyzwaniem jest także przetwarzanie różnorodnych foramtów danych, które często wymagją zaawansowanych technik parsowania, gdyż modele nie są w stanie ich prprawnie przetwarzać. \n",
    "5. How can the system be improved to handle complex queries requiring multiple document lookups?\n",
    "\n",
    "   System RAG można usprawnić, aby lepiej radził sobie ze złożonymi zapytaniami wymagającymi wyszukiwania w wielu dokumentach poprzez zastosowanie wyszukiwania iteracyjnego (Multi-Hop Retrieval), które polega na wieloetapowym procesie przeszukiwania. W tej metodzie system najpierw przeprowadza początkowe wyszukiwanie, aby znaleźć podstawowe informacje związane z zapytaniem. Następnie, na podstawie wyników tego pierwszego wyszukiwania, formułowane są kolejne, bardziej szczegółowe podzapytania, które prowadzą do dalszych etapów przeszukiwania dokumentów.\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
